<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Deciphering Big Data - Career</title><link href="https://decipheringbigdata.com/" rel="alternate"></link><link href="https://decipheringbigdata.com/feeds/career.atom.xml" rel="self"></link><id>https://decipheringbigdata.com/</id><updated>2022-05-22T14:36:00-07:00</updated><subtitle>Description of the site</subtitle><entry><title>What To Learn To Become a Data Engineer</title><link href="https://decipheringbigdata.com/what-to-learn-to-become-a-data-engineer.html" rel="alternate"></link><published>2022-05-22T14:36:00-07:00</published><updated>2022-05-22T14:36:00-07:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.com,2022-05-22:/what-to-learn-to-become-a-data-engineer.html</id><summary type="html">&lt;p class="first last"&gt;Explaining the skills you need to master in order to become a data engineer&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="what-even-is-a-data-engineer"&gt;
&lt;h2&gt;What even is a data engineer?&lt;/h2&gt;
&lt;p&gt;It's not so straightforward! You see, similar to data scientist, the role of data engineer is quite new to the tech scene. There isn't as much uniformity in responsibilities as there is with software engineers, which is a role that has been around for much longer. There are some common skills that most data engineers out there share but what you actually end up doing and therefore need to be familiar with entirely depends on the company and in some cases, the team within that company.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="skills-to-learn"&gt;
&lt;h2&gt;Skills to learn&lt;/h2&gt;
&lt;div class="section" id="fundamental-skills"&gt;
&lt;h3&gt;Fundamental skills&lt;/h3&gt;
&lt;p&gt;These are the skills that are common among most data engineers regardless of whether they're more on the analytical/product side or the engineering side of the spectrum.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;SQL&lt;/strong&gt; - working in data and not knowing SQL is a cardinal sin! Not knowing SQL as a data professional is like working as a professional musician, but not knowing how to read or write music. You could probably do it if you're good at playing your instrument, but it would seriously hinder you when working with other professional musicians. Simply put, SQL is the language of data. Even if you as a data engineer build pipelines and datasets with a programming language like Python or Scala, your peers over in analytics, finance, marketing, etc. will probably be using SQL to query the data. Not being able to work with them because you're not familiar with SQL is a problem you want to avoid.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data reasoning&lt;/strong&gt; - you need to understand how data are used to inform decisions. What are the key metrics you can derive from raw data by curating it? What aggregations and segmenting do you need to apply to the data to answer business questions? How can the data be best represented? These are key questions that you should know the answer to as a data engineer because it's your north star. The whole point of collecting data and processing it is to make it usable for people. This may involve knowing a little math and statistics, being familiar with basic visualization techniques, and knowing how to tell a story with data, even if you're not directly doing those things.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data modeling&lt;/strong&gt; - to make data usable and easy to work with for others, you need to model it. There are many approaches out there depending on how the data will be used. For example, the way you model data in a transactional system may be very different than in a purely analytical setting. You as a data engineer need to be aware of how the data will be consumed, which will then inform the degree of &lt;a class="reference external" href="https://decipheringbigdata.com/be-normal.html"&gt;normalization&lt;/a&gt; that needs to be applied and whether to leave your data in a very raw state or to curate it heavily.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pipeline orchestration&lt;/strong&gt; - this is how you make the classic ETL (extract, transform, load) process happen. Every stage in your pipeline needs to be orchestrated, i.e. run by some system. Again, there are many ways and tools out there to do this. Airflow is a commonly used tool, but you may also find yourself running scripts on your own servers or using serverless solutions like &lt;a class="reference external" href="https://decipheringbigdata.com/my-friend-sam.html"&gt;AWS Lambda&lt;/a&gt; to run your workflows. Regardless of which tool you use, the principles are the same. You break your pipeline up into different stages and run them sequentially or out of order on a cadence or in real time, depending on how much data you need to ingest, how frequently it needs to be ingested, how much processing needs to be done, and where you need to load it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data privacy&lt;/strong&gt; - the privacy of customers from whom you collect data is paramount to anything else, but it's often a lower priority for engineers who just want to build cool solutions. Nowadays there are many different regulations (GDPR, CCPA, CPRA, etc.) that directly dictate what data you can collect and how you can use that data in your business. Violations of these regulations can have a significant negative impact to the business and the individuals they protect. Since data engineering teams are often the first stop for data in an organization, you need to be aware of them so you can design your solutions to protect that privacy and avoid any repercussions from storing and making restricted data available downstream.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="analytical-skills"&gt;
&lt;h3&gt;Analytical Skills&lt;/h3&gt;
&lt;p&gt;Some data engineers may specialize more on directly enabling analytical capabilities in their organization rather than handing off the data to data analysts or scientists to put into action. They may be embedded within a data analytics, finance, product, or marketing team for example and may be directly responsible for building reporting and highly specific business metrics on top of data that has already been processed upstream. This side of data engineering is branching more into what's being called &amp;quot;Analytics Engineering&amp;quot; but the role is nothing new. It has been around for a long time in larger companies as simply a product data engineer or a business intelligence engineer. This is quite common at places like Amazon and Meta, where data engineers tend be more involved with report and dashboard building as well as answering business questions with complex analytical queries. These data engineers tend to come from analytical backgrounds or classic SQL developer roles and rely more on the following skills.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Summarizing data and reporting&lt;/strong&gt; - knowing how to build reports and massage the data to tell a story. The data you summarize in dashboards and reports will be consumed by business leaders who need accurate insights quickly in order to monitor the business and make strategic decisions. Don't underestimate this skill! It can take many years to build a knack for how to report data in a fast-paced business setting. It's also highly dependent on the type of organization you're in. For example, at Amazon, giant docs full of black-and-white Excel tables packed with numbers is the norm. At other places like Meta, the dense docs are replaced by streamlined and interactive dashboards that are always online and available for users to click through and digest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data visualization&lt;/strong&gt; - visualizing data is an effective way to digest it and tell a story. I probably don't need to tell you how common charts are in the business world because they're everywhere! So any engineer directly involved in communicating data needs to be aware of how best to visualize that data and which tools to use. This is also a commonly underestimated skill, but there are definitely right and wrong ways to visualize data. Knowing the difference can take a long time to master.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advanced SQL&lt;/strong&gt; - in more analytical settings, the metrics can get quite complex. They may need to be derived by joining and aggregating many different tables across the company or by applying complicated business rules. As a result, the SQL required is often more advanced than in other areas of data engineering where you may just need SQL to do straightforward operations. They may involve aggregating over wide time periods like multiple quarters or years to report on trends over time, which typically involves larger amounts of data. The queries will be larger overall and may involve working through different aggregations of data as you combine all of the tables you need. So you need to be more cognizant of how to write performant SQL on the platform you're using and be able to break the problem up into multiple processing stages before coming up with a final query to produce the metrics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Basic statistical modeling and inference&lt;/strong&gt; - drawing conclusions from data after it has been made available, curated, and visualized is a complex process by itself. How do you know whether a trend is expected or not? How can you tell with enough confidence that your product or business decision is actually making a difference in the outcome you're measuring given the data at hand? How much data do you even need to collect in order make a reasonable conclusion? How can you set up tests to validate your hypotheses? These are common problems in the field of statistics. Being familiar with this field and how to approach problems like these can take you a long way in reasoning about the data you have. Even if as a data engineer you're not a statistics expert like your coworkers over in data science, just having a fundamental understanding of it can help you produce better data products for your stakeholders.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="advanced-engineering-skills"&gt;
&lt;h3&gt;Advanced Engineering Skills&lt;/h3&gt;
&lt;p&gt;Now some data engineers may actually be building end-to-end data ingestion and processing systems that share similar characteristics to those traditionally tackled by software engineers. In this context, data engineers are like applied software engineers who focus specifically on data. They may be writing both the orchestration and business logic code to actually ingest and process raw data or building and maintaining the platforms that enable others in the organizations to do so. Other titles you may come across for these types of data engineers are software engineers - data, data platform engineers, and data software engineers. They need to be very familiar with the following concepts.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; - a general purpose programming language that can literally do anything - data analytics, statistical computing, machine learning, object oriented programming, functional programming, web development, you name it. Many developers who grew up writing C/C++ and Java tend not to like Python because it's not statically typed and since it's an interpreted language that's &lt;a class="reference external" href="https://decipheringbigdata.com/python-parallelism.html"&gt;not truly multi-threaded&lt;/a&gt;, it's pretty slow. However, there is no getting around the fact that it's extremely popular and used heavily by most data teams these days. So if you're going to be working as an engineer with a focus on data, you need to know how to at least read Python, which is actually just like reading pseudocode :). Also, Airflow is a very popular orchestration tool that is written in Python and the way you typically write Airflow DAGs is with Python. The odds that you will come across this tool are very high.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JVM language of choice&lt;/strong&gt; - now Python is still somewhat of a second-class citizen with modern data processing frameworks. Tools in the Hadoop ecosystem and modern alternatives like Apache Spark, Flink, Beam, etc. are mostly developed using Java and Scala, which run on the Java virtual machine (JVM). The APIs and documentation tend to be available in Java or Scala first before they're ported over to Python. But usually that's not a big issue unless you're working with super new features. However, even if the APIs are similar and the same core features of these frameworks are available across the board, you will find that having an understanding of Java and Scala will help you discern how these frameworks work to process your data better, which is particularly handy during troubleshooting. Highly specialized data engineering teams that work on big data systems also tend to write their code in Java or Scala because it's a better choice for large code bases and typically results in more efficient code. These engineers may also come from traditional software engineering backgrounds where Java has dominated for a long time. The static typing also really helps when you're writing Spark code. It's better to find an error in your code at compile time than 30 minutes into your job.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed computing&lt;/strong&gt; - all of the big data frameworks have this concept at their core. Understanding how data are distributed and processed in parallel is critical to truly grasp what's going on in your pipelines. If you have a good understanding of this, you'll be able to to design more efficient pipelines and have an easier time troubleshooting issues with them. So take the time to learn how the way you store your data, your partitioning schemes, indexing, and operations on the data affect how it's distributed at runtime.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming architectures&lt;/strong&gt; - the world of data processing is moving more from a batch to a streaming paradigm. If you can write a pipeline as a streaming job, it can be run both on a set batch schedule or in real-time with just a simple config change instead of a rewrite of the code. The same is not true the other way around. Streaming jobs tend to be simpler and require less orchestration complexity. The streaming frameworks can simply listen to any new files continuously or since they last ran, compare with a checkpoint to determine what needs processing, and carry out the work without anything else telling them what to process. This approach is also becoming easier to adopt with platforms like Databricks doing a lot of the work for us behind the scenes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Devops&lt;/strong&gt; - having an understanding of how to deploy your data systems and how to efficiently manage the associated infrastructure is critical. This means being familiar with continuous integration and deployment tools, infrastructure as code, containerization, monitoring tools, and security. I know some people may have dedicated devops teams that will handle a lot of this for them, but knowing how to tackle it yourself if you need to is important. I've seen too many data engineers manually creating AWS artifacts, manually editing IAM policies, not having a good grasp of all of the access tokens out in the wild, manually pushing their pipelines to production, etc. If you find yourself doing those types of tasks frequently, find some time to learn proper devops practices. It will save you a ton of time and headaches in the future!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud computing&lt;/strong&gt; - this is another critical skill for engineers focusing on data. Unless you're working for a larger and more traditional company that may still run their data stack on on-premise hardware, odds are that you will be using some sort of cloud computing provider. The most commons providers are AWS, Azure, and Google Cloud. They all have specific products with different names but the core types of these products are the same - some sort of object storage like S3 and Google Storage, server instances like EC2, data warehouses like Redshift and BigQuery, serverless tools like Lambda and Azure Functions, and managed hadoop frameworks like EMR. These are typically the services most used for data engineering so picking one cloud provider and understanding how to use these services on their platform will take you far.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Version control&lt;/strong&gt; - then if you're working on any engineering team, you will need to know how to use version control software. There are many out there but the most common is Git. There are also multiple providers that offer Git services - Github, Bitbucket, Gitlab, etc. - but Git itself is the same open-source system common to all of them. It can take a while to grasp the concept and be exposed to all of the different scenarios you may encounter when working on a codebase that has many contributors, but putting in the effort to learn it and learn it well will make you a much more productive engineer. Version control is less common with the more analytical types of data engineering roles where engineers may just be writing their code directly into some UI, but it's increasingly being adopted more in those less technical roles with the help of frameworks like dbt.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-learn-these-skills"&gt;
&lt;h2&gt;How to learn these skills&lt;/h2&gt;
&lt;p&gt;It's a long list for sure, but don't be intimidated! Most professionals don't truly master all of these and it can take many years to be comfortable with a good chunk of this list. Start off with the fundamentals - learn how to write basic ETL using Python and SQL commands. Learn SQL well and write basic Python scripts to run simulated jobs. If you don't have a database handy, use libraries like Pandas in Python and text or JSON files that are freely available online from many sources like data.gov to practice your data wrangling skills. Ask yourself questions about the data and try to answer them. Explore as much data as you can using these tools. Practice is the only way to build up your data intuition skills. Also, try modeling the data. How can you structure it in order to make it easy for people to work with it? Try to apply concepts like normalization and star and snowflake schemas to your data.&lt;/p&gt;
&lt;p&gt;Once you have a good grasp of the fundamentals, start playing around with more advanced engineering concepts like cloud computing. Amazon AWS is has plenty of free walkthroughs online on how to create your own databases, use S3 as object storage, query that data with Athena, launch EMR clusters, and pretty much everything you can think of to take your engineering knowledge to new heights. With EMR clusters, you can launch fully managed distributed environments to run Spark in. You don't need that to learn Spark of course because you can just install it locally and run it on your own computer. But, once you get a hang for the syntax and the data frame + Spark SQL APIs, using a tool like EMR is a good way to apply that knowledge on a larger scale of data. Google is your friend here! Just try it out and work your way through the common pitfalls.&lt;/p&gt;
&lt;p&gt;Learn about partitioning data in object storage using Hive formatting and how Spark is able to discover the data and read it. Understand how the number of files and individual file sizes impact performance. That alone will take you farther than most data engineers out there who have dedicated teams in their companies worry about all of this infrastructure and data storage stuff for them or who don't really understand how the internals of Spark work.&lt;/p&gt;
&lt;p&gt;Once you have a good grasp of batch processing pipelines on both databases and object storage using Python, SQL, and Spark, you may want to venture into writing streaming jobs. Learn how to work with the Spark Structured Streaming API using real-world feeds like what Twitter offers. You can even set up your own Kafka cluster or use AWS Kinesis and publish dummy data records into streams that you then consume and process with a streaming framework of your choice.&lt;/p&gt;
&lt;p&gt;The important theme here is to just start! You have all of the resources available to you. There is no need to actually work in data to practice these skills. There is plenty of data online and many of the popular tools are completely open source. Below are a few of my posts to guide you in the right direction:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;An &lt;a class="reference external" href="https://decipheringbigdata.com/mysql-yoursql-oursql-nosql.html"&gt;overview&lt;/a&gt; on different data storage solutions you can learn about&lt;/li&gt;
&lt;li&gt;An intro to &lt;a class="reference external" href="https://decipheringbigdata.com/be-normal.html"&gt;normalization&lt;/a&gt; and &lt;a class="reference external" href="https://decipheringbigdata.com/stars-and-snowflakes.html"&gt;star schemas&lt;/a&gt; that can come in handy when modeling data&lt;/li&gt;
&lt;li&gt;Using &lt;a class="reference external" href="https://decipheringbigdata.com/my-friend-sam.html"&gt;AWS Lambda&lt;/a&gt; as an orchestration tool to ingest data from Twitter on a schedule&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="Career"></category><category term="Career"></category></entry><entry><title>Data Engineering at FAANG</title><link href="https://decipheringbigdata.com/data-engineering-at-faang.html" rel="alternate"></link><published>2021-03-28T00:00:00-07:00</published><updated>2021-03-28T00:00:00-07:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.com,2021-03-28:/data-engineering-at-faang.html</id><summary type="html">&lt;p class="first last"&gt;The top companies may not necessarily provide the top opportunities&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="my-experience"&gt;
&lt;h2&gt;My Experience&lt;/h2&gt;
&lt;p&gt;FAANG stands for Facebook, Amazon, Apple, Netflix, and Google/Alphabet. These companies are often seen as the holy grail
of achievement among engineering graduates. The top of top for career opportunities and compensation. New graduates flock
to these companies looking to be challenged, build cutting edge products, and add the prestigious brand names to their
resumes. But are roles in these companies really as great as they sound?&lt;/p&gt;
&lt;p&gt;I've worked at these types of companies in the Seattle area for about 3-4 years, rotating between 4 different teams and
leaving to work at a start up only to return 7 months later. I've also had the opportunity to work in different industries
including banking, ad-tech, and manufacturing in both small companies and large corporations. I will share with you what
I've observed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="internal-tools-galore"&gt;
&lt;h2&gt;Internal Tools Galore&lt;/h2&gt;
&lt;p&gt;My biggest pet peeve about my job is the amount of internal tools I have to deal with on a daily basis. These tools
are supposed to make our lives easier and streamline the development process but from my experience they abstract away
crucial details about the underlying services that engineers should know. Sometimes these services may be industry
standard like AWS, Kubernetes, continuous deployment tools, etc. but because you have the internal applications sitting
on top of these services and only a shitty proprietary UI is exposed to you, in the end you don't really learn how to work
with AWS, Kubernetes, or anything else. You learn to work with the internal tools. You can't really put that on your
resume when applying to bigger and better opportunities down the line and most importantly, it robs you from the opportunity
to learn valuable skills on the job.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="rampant-bureaucracy"&gt;
&lt;h2&gt;Rampant Bureaucracy&lt;/h2&gt;
&lt;p&gt;There is a lot of corporate red tape at FAANG companies. Teams like to build walls around their work and decisions are
made at a very slow pace. Often any meaningful change involves dealing with strong personalities and defensive leaders of different
teams. That may be interesting to you if you enjoy the challenge of dealing with different types of people and working
through the bullshit to drive change or whatever. However, if you just want to work on interesting projects as an engineer,
often times the bureaucracy can be a real pain. It means unnecessary meetings, ego-stroking, way too much planning without
any execution, the necessity to constantly update others on your progress, product managers over-promising on your behalf
to impress the leaders, etc. Worst of all it can also mean that sometimes you just can't touch what you really want to
gain more exposure to because some other team owns it. Whether it's getting access to some code or contributing to a certain
pipeline, the red tape can make it so difficult to do anything beyond your immediate job responsibilities that it can
seriously hamper your growth as an engineer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="easy-to-get-stuck-in-ops-hell"&gt;
&lt;h2&gt;Easy to Get Stuck in Ops Hell&lt;/h2&gt;
&lt;p&gt;I've been in teams that very rarely write any new code or release incremental features. All they do is hit buttons on some
UI to restart failed jobs, copy existing jobs to run in new regions, dig into tickets regarding those jobs, and answer
e-mails about the jobs. Essentially just maintaining the same old pipelines that may add a lot of value to the company but
are just plain boring to work with. Months go by and you realize you've been doing the same ops work over and over without
learning anything. This isn't even unique to data engineering, which tends to be more ops-heavy at large companies. I've
even been in software engineering teams that do the same thing.&lt;/p&gt;
&lt;p&gt;This is surprisingly common given how many things have been built at these companies. Rarely do you get to be in a team
that's truly pushing the boundary and working in a completely greenfield environment. In other words, those truly
innovative teams that everyone thinks they'll be a part of if they join a prestigious tech company are more of an
exception than the norm. It's great if you can be a part of those teams, but it's unlikely that you will be from the start.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="an-unhealthy-focus-on-job-levels"&gt;
&lt;h2&gt;An Unhealthy Focus on Job Levels&lt;/h2&gt;
&lt;p&gt;I don't know if it's selection bias with the types of candidates that FAANG attracts but there is a questionable obsession
with levels, titles and total compensation at these companies. If you don't believe me, spend a few minutes reading
through FAANG posts on &lt;a class="reference external" href="https://www.teamblind.com/"&gt;https://www.teamblind.com/&lt;/a&gt;. Sometimes it feels like that's all people care about. I have definitely
felt that sometimes people prioritize the projects that will get them noticed by some leaders with fancy titles over anything
else, even if those projects will not help them learn anything, grow in any other way, or add the most value in the long run.
I've even had managers insinuate that the work their reports are doing is not as important as pleasing some director or VP,
even if that work is pushing buttons on some UI and you have 4 talented software engineers wasting their time hitting those buttons.&lt;/p&gt;
&lt;p&gt;My first job out of college was in banking where brown-nosing and kissing your superior's ass is the norm. People make
entire careers out of that alone. It's one of the reasons why I left that industry. I felt like it didn't matter what I knew
as long as I impressed the right people one way or another. I thought that big tech would be different and to some extent
it is, but there is still a lot of that unhealthy obsession with pleasing authority figures at all costs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="projects-lacking-in-scope"&gt;
&lt;h2&gt;Projects Lacking in Scope&lt;/h2&gt;
&lt;p&gt;Most of the cool data engineering stuff you learn about in school or in your spare time outside of work has already
been done at FAANG companies. They pioneered the application of that fancy machine learning algorithm or way to process
data. Odds are that you will not be working on any of that because it simply has already been done. What does need to get done
is system maintenance and super downstream work of slicing and dicing some datasets that have already been curated by
a different team long ago. It's common to come across long chains of teams doing work that's frankly not that interesting
from a technical perspective. You depend on that work from a different team and some other team depends on your small piece
of the puzzle to inform their business decisions. There is a lot of simple SQL writing and very little system design work.&lt;/p&gt;
&lt;p&gt;If you care more about the business or doing your job well and getting promoted, then that type of work can be great.
However, if you're looking for a technical challenge after learning all about the latest in streaming data, artificial intelligence,
data modeling, or some other hot topic, you may be disappointed in the types of projects you actually end up working on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="is-it-worth-it"&gt;
&lt;h2&gt;Is It Worth It?&lt;/h2&gt;
&lt;p&gt;I am grateful for the opportunities that I've had and wouldn't do anything different if given the chance to do it all
over again. So, yes I do think it's worth it and I recommend these companies to any new grad looking to gain valuable
real world experience and make good money while doing so. However, I do not recommend getting stuck in dead-end roles
or forgoing opportunities to learn just because you've made it to FAANG. Don't be afraid to job hop and search for roles
that will make you a better engineer in the long run, even if you have to take a pay cut.&lt;/p&gt;
&lt;p&gt;While start ups or lesser known companies may not offer the same level of compensation as FAANG, you may end up learning
a lot more in a shorter amount of time, which will ultimately lead to better opportunities and more compensation down
the line. This is because younger and/or smaller companies may not suffer from the same issues that plague larger more
established corporations like FAANG. No matter what the recruiters or Kool-Aid drinkers at these companies say, the truth
is that they face the same issues that exist in other non-tech bloated corporations.&lt;/p&gt;
&lt;p&gt;So to summarize, don't chase the brand name. Look for the roles that will teach you the most and give you plenty of
opportunity to build new things. Also, do your due diligence before accepting a role. Talk to members of the team and
ask them what they do every day and if they enjoy their jobs. Pretty much all of them will say they do because they're
trying to get you to join but you can often times read between the lines and deduce from their daily tasks what
the job will be like.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="there-is-no-substitute-for-self-learning"&gt;
&lt;h2&gt;There Is No Substitute for Self-Learning&lt;/h2&gt;
&lt;p&gt;In my opinion the most important thing an engineer could do in their early career is learn as much as possible. While
I think that younger companies offer better opportunities to learn than FAANG, ultimately no company will provide you
with the same growth as self-learning will. When you take the initiative to learn new things as a result of your own
passion and curiosity you can learn a lot more in a shorter amount of time than you can at work. This is because you won't
be hindered by formal procedures, company priorities, operational load, or your official title.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Career"></category><category term="Career"></category></entry><entry><title>Tech Interviews Are Broken</title><link href="https://decipheringbigdata.com/tech-interviews-are-broken.html" rel="alternate"></link><published>2021-01-18T00:00:00-08:00</published><updated>2021-01-18T00:00:00-08:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.com,2021-01-18:/tech-interviews-are-broken.html</id><summary type="html">&lt;p class="first last"&gt;Do you even LeetCode bro?&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="exhibit-a"&gt;
&lt;h2&gt;Exhibit A&lt;/h2&gt;
&lt;p&gt;Did you know that there are companies out there dedicated to nothing more than helping you prepare to pass a tech
interview? Below are some of the ones I've heard of recently.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.interviewkickstart.com/guide/interview-kickstart-cost/"&gt;Interview Kickstart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.outco.io/"&gt;Outco&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.algoexpert.io/purchase"&gt;AlgoExpert&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Look I understand that data structures and algorithms are an important foundation to any job that requires serious
system design and development. I am a huge advocate for truly understanding the concepts at their core rather than
focusing on the superficial surface-level frameworks and such. However, there has to be something wrong with your process
when there is a whole industry dedicated to helping people answer your interview questions.&lt;/p&gt;
&lt;p&gt;Tech companies are signaling that being able to answer coding puzzles that require you to spend weeks to months reading
interview prep books and spending thousands of dollars on classes from the institutions above is more important than
actually being a good developer. I say this as both someone who has been rejected from roles that I would have been able to
add value in and who has interviewed candidates for tech roles at big tech but had to turn them down despite
their valuable levels of experience.&lt;/p&gt;
&lt;p&gt;LeetCode trumps experience every time. You could have literally created an application that is used by most of the people
in the prospective company. It doesn't matter if you can't also reverse a linked list from muscle memory. Case and point
with this Tweet by the creator of Homebrew.&lt;/p&gt;
&lt;img alt="Max Howell on Google's interviewing methodology" src="/static/post12/post12_homebrew.png" style="width: 100%;" /&gt;
&lt;/div&gt;
&lt;div class="section" id="so-should-we-just-suck-it-up"&gt;
&lt;h2&gt;So Should We Just Suck It Up?&lt;/h2&gt;
&lt;p&gt;While it's relatively easy to spend a few weeks or months reviewing old concepts you never have to implement from
scratch on the job like binary trees, heaps, tries, etc. I think it adds absolutely no value whatsoever and it detracts
from what's really important. These are my reasons:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. A full-time software engineering job should be enough to get another engineering job that requires the same level
of experience&lt;/strong&gt;, especially if the person you're interviewing has actual code to demonstrate and designs to walk through.
I don't know about you but I have other shit to do outside of work than prepare for interviews. I could be playing an instrument,
exercising, bonding with my family, walking my dog, hell, even building actual software products just like I would be doing
on the job. All of these are much better uses of my time than reviewing tricky concepts that are hardly ever used in
practice like dynamic programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. We should incentivize candidates to build solutions to actual real world problems rather than solving toy whiteboard
puzzles.&lt;/strong&gt; Imagine if instead of grinding through LeetCode questions to become more cogs in the big tech machine,
all of those talented and passionate kids were building real world applications. I think the industry would be much better
off and there would be more helpful resources out there to judge candidates' talents and experience levels with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Proponents of the current interviewing system insist that the process measures problem solving skills, but from my
experience the problem solving done on the job is nothing like what is measured by the puzzle-like DS/Algo interview
questions.&lt;/strong&gt; The problem solving on the job involves researching new libraries and technologies, dealing with ambiguity,
collaborating with other people, deep diving data to root cause issues, being able to unblock yourself, and managing time
effectively. That's literally 95-99% of what's required. I have never found myself stuck on some mind-bending algorithm
that needs to be implemented right away (and preferably on a whiteboard without referencing any resources). I have never
been stuck on any coding issue unless it's related to not fully understanding how a library or system works. The current
interviews don't measure a candidate's ability to get through those types of problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Having a good foundational understanding of algorithms and data structures is not enough to pass whiteboard tech
interview problems.&lt;/strong&gt; There is a large &amp;quot;gotcha&amp;quot; component to these questions where simply understanding the foundation
(which data structure to use for what and how to optimize runtime/space complexity given the problem at hand) is not
enough. Given the amount of time that you're allotted to solve those tricky questions it's not possible for most good
engineers to solve them optimally without going through a bunch of similar questions beforehand, including the interviewers
themselves. I know this because I am okay at my job (haven't been fired yet in the 4 or so years of technical
experience under my belt) and I can't consistently solve those tricky questions without extensive review beforehand. The majority of
engineers I work with are also not super geniuses who can take a difficult LeetCode question and solve it on a whiteboard
within 30 minutes without reviewing beforehand. That review requirement is a problem because of reasons 1-3 above.&lt;/p&gt;
&lt;p&gt;So no, we should change the process.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-better-solution"&gt;
&lt;h2&gt;A Better Solution&lt;/h2&gt;
&lt;p&gt;Have the candidates implement a short coding project and write a design doc explaining why they structured it the way they
did and how they would deploy the application and maintain it going forward. Instead of having interviewers give whiteboard
interviews, have them review the code and designs beforehand and ask them relevant questions about it when they speak to
them in person. Ask questions like what metrics they would collect to measure the state of the application, how would
they scale it to X amount of users, how would they troubleshoot a particular issue, etc.&lt;/p&gt;
&lt;p&gt;The complexity of these projects and related questions will vary depending on how much experience the candidate has.
Obviously if it's an experienced candidate the interview should mostly focus on previous projects they have worked on.
It's quite easy to tell whether someone knows what they're talking about just by having a technical conversation with them
in person.&lt;/p&gt;
&lt;p&gt;The in-person interviews should also involve peer coding questions in a similar environment the candidate would find themselves
in on the job. That means with an IDE and full access to Internet resources, wikis, documentation, etc. If they get stuck
they should be able to look for help online because let's face it, that's one of the most valuable skills a developer could
have. The questions themselves should be relevant to the job. If the job requires the candidate to glue a bunch of API
endpoints together then ask them to do that. If a job requires building a frontend then have them build a frontend UI.
If the job requires writing Spark applications and orchestrating big data jobs then let them show what they got by doing
just that. When they're finished have them explain their code and ask probing questions about how it would be
deployed as part of a scalable system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="it-won-t-change-anytime-soon"&gt;
&lt;h2&gt;It Won't Change Anytime Soon&lt;/h2&gt;
&lt;p&gt;At least not at big tech companies like FAANG. They just have way too much demand and from their perspective, it's better
to turn away a good candidate that didn't review DS/Algorithms beforehand than bring in a bad candidate who doesn't
actually know what they're doing. That's why they choose to focus on these types of questions. Not so much because they're
good measures of problem solving ability but because they're effective weed-out tools to narrow down the applicant pool
to manageable levels. I understand if we're talking about interviewing fresh college grads who haven't done anything.
It makes absolutely no sense to me when it comes to experienced candidates, but alas that's how it is.&lt;/p&gt;
&lt;p&gt;The engineers doing all of the hiring in these companies also tend to give in to industry standards and think it's just
the way it has always been so there's no point in changing. Some even wear their LeetCode ability like
a badge of honor, looking down upon anyone not willing to put in the dedication to solve those questions themselves. It's
quite toxic actually. I've interviewed super qualified candidates before that would have been great assets but my peers
didn't think so because the candidates struggled with esoteric DS questions. I've literally read in the feedback
comments for candidates with years of software experience, &amp;quot;struggled with [insert hard LeetCode question here], not good
at coding,&amp;quot; which just boggles my mind. How can you make such a determination from not being able to answer those types of
questions on a whiteboard? Did you even talk to the candidate about their many years of relevant experience and obvious
actual engineering ability?&lt;/p&gt;
&lt;p&gt;It's a shame because I think those companies would certainly be better off if their interview process
were more like the actual jobs the candidates need to do once they join. Anyway, hit me up on LeetCode fam -
&lt;a class="reference external" href="https://leetcode.com/adaros92/"&gt;https://leetcode.com/adaros92/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Career"></category><category term="Career"></category></entry><entry><title>The Role of the Data Engineer</title><link href="https://decipheringbigdata.com/the-role-of-the-data-engineer.html" rel="alternate"></link><published>2021-01-03T00:00:00-08:00</published><updated>2021-01-03T00:00:00-08:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.com,2021-01-03:/the-role-of-the-data-engineer.html</id><summary type="html">&lt;p class="first last"&gt;Data engineers should do more than just build and maintain ETL jobs&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="what-data-engineers-actually-do"&gt;
&lt;h2&gt;What Data Engineers Actually Do&lt;/h2&gt;
&lt;p&gt;The textbook definition of a data engineer is a person who builds pipelines that prepares data for scientists to consume.
This is true to some extent but in the real world, the role varies widely.&lt;/p&gt;
&lt;p&gt;From my experience, the data engineering crowd is composed of the following members.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Old school BI/ETL engineers&lt;/li&gt;
&lt;li&gt;Younger professionals who sought out the role lured by the shiny big data frameworks and DS hype&lt;/li&gt;
&lt;li&gt;Seasoned software engineers who chose to specialize in big data (or were tasked with a problem and had no other choice)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Just like the crowd, the roles are mixed as well.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Traditional data warehousing with drag and drop ETL tools like Informatica&lt;/li&gt;
&lt;li&gt;Writing SQL queries to extract data from a data warehouse&lt;/li&gt;
&lt;li&gt;Building custom ETL orchestration with programmatic frameworks like Airflow in languages like Python/Java/Scala&lt;/li&gt;
&lt;li&gt;Big data processing and storage using Spark/MapReduce with languages like Python/Java/Scala on cloud technologies/on-prem Hadoop clusters&lt;/li&gt;
&lt;li&gt;Building scalable data processing systems and infrastructure to automate the above and/or enable self-service access to data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What you end up doing largely depends on the team you're on and what the needs are. However, I think we can all agree
on which roles are the most interesting to engineers if engineers are what companies actually want when they hire DEs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="moving-data-from-point-a-to-b-is-not-fun"&gt;
&lt;h2&gt;Moving Data From Point A to B Is Not Fun&lt;/h2&gt;
&lt;p&gt;If you think the textbook definition of a data engineer sounds boring it's because it is. People don't go into data
engineering because they like to set up an ETL to join a few tables, aggregate some columns, and spit the result out to
the analysts. Certainly not good engineers anyway.&lt;/p&gt;
&lt;p&gt;Yet the expectation from most of my customers assumes that we enjoy building ETLs and solely exist to hand over the
precious data upon request. This is a silly expectation for a few reasons.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;If you can get a Master's or PhD in Math/Econ/Stats/whatever and be a fancy data scientist you can also write SQL&lt;/li&gt;
&lt;li&gt;Given that the process of transforming data is tedious, it will never be the end goal for talented engineers and they will most likely get bored and leave&lt;/li&gt;
&lt;li&gt;Enforcing a contract where data engineers provide and data scientists receive sets up the system so that no one is building anything end-to-end, which is not rewarding to anyone involved&lt;/li&gt;
&lt;li&gt;Enforcing this contract also leads to increased latency from scientists sitting around waiting for data engineers to prioritize their work&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="data-engineers-should-focus-on-the-infra"&gt;
&lt;h2&gt;Data Engineers Should Focus on the Infra&lt;/h2&gt;
&lt;p&gt;Instead of building and maintaining the ETL, engineers should instead focus on enabling self-service and building sound infrastructure. There
is no reason why scientists can't structure and pull their own data. They should be empowered to own the entire analytics
process end-to-end and engineers should be given the time to make this as easy and fool-proof as possible.&lt;/p&gt;
&lt;p&gt;This means cataloguing the data, building automatic alarming and outlier detection systems, optimizing bottleneck pipelines,
improving data quality, enforcing data governance processes, building self-service tools to remove friction, and implementing
complex data pipelines that require more than just SQL.&lt;/p&gt;
&lt;p&gt;Unless the data is in a super raw format or needs to be processed with very little latency, specialized knowledge in
data processing and modeling is not required. The tools exist so that data does not really need to be structured in a data warehouse
to be useful to analysts. If you want to query those JSON logs in S3 you can. The same is true for denormalized parquet files or
raw CSVs. Data engineers should build the right abstractions that leverage these tools and make new ones available instead of
being stuck maintaining a bunch of ETL jobs that the scientists can own themselves.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-future-of-the-role"&gt;
&lt;h2&gt;The Future of the Role&lt;/h2&gt;
&lt;p&gt;With third-party ETL tools becoming more sophisticated and cloud computing providers making more automation and data discovery
services available, data engineers will be taking on more engineering and infrastructure heavy roles. The role of a data engineer
should be seen as another branch of software engineering requiring the same fundamental way of thinking. They will need to
adopt the same software and system design practices as software engineers but use them to build scalable and intuitive
data infrastructures.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Career"></category><category term="Career"></category></entry></feed>