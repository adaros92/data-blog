<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Deciphering Big Data</title><link href="https://decipheringbigdata.net/" rel="alternate"></link><link href="https://decipheringbigdata.net/feeds/all.atom.xml" rel="self"></link><id>https://decipheringbigdata.net/</id><updated>2020-12-13T00:00:00-08:00</updated><entry><title>Stars and Snowflakes</title><link href="https://decipheringbigdata.net/stars-and-snowflakes.html" rel="alternate"></link><published>2020-12-13T00:00:00-08:00</published><updated>2020-12-13T00:00:00-08:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.net,2020-12-13:/stars-and-snowflakes.html</id><summary type="html">&lt;p class="first last"&gt;Structuring data in OLAP systems&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="requirements-in-an-olap-system"&gt;
&lt;h2&gt;Requirements in an OLAP System&lt;/h2&gt;
&lt;p&gt;In my previous &lt;a class="reference external" href="be-normal.html"&gt;Be Normal&lt;/a&gt; post I mentioned that normalization is good for OLTP but not so
much for OLAP systems. The way you adapt data models for OLAP purposes is by de-normalizing tables in a normalized
data model. At a high level this process involves precomputing certain features and joining tables together for our
analyst customers.&lt;/p&gt;
&lt;p&gt;Analysts typically need to answer complex business questions dealing with certain &amp;quot;facts&amp;quot; about a business. For example,
what do revenues look like over time? What category of product sells the best during sale periods? Which apps are the
top performing as measured by daily active users? With a highly normalized database, the analysts would have to write
complex queries to join many different tables together and aggregate up individual records to produce the summarized
figures they need in order to answer these questions.&lt;/p&gt;
&lt;p&gt;Here I discuss how we may go about making this process easier by designing an OLAP data model with the help of two buzz
phrases in the data modeling community: star and snowflake schemas.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="star-schemas"&gt;
&lt;h2&gt;Star Schemas&lt;/h2&gt;
&lt;p&gt;The basic premise of a star schema is to organize your data such that you store your key business measures that analysts
care about in one or many fact tables. How granular you go with these measures is really up to the needs of the business
and what types of questions we want to answer with the data.&lt;/p&gt;
&lt;p&gt;These fact tables will then have &lt;a class="reference external" href="https://www.techopedia.com/definition/7272/foreign-keyl"&gt;foreign keys&lt;/a&gt; to other
tables in the database that describe the numerical facts stored in the fact tables. These types of tables are called
dimensions and commonly deal with locations, time, products, organizations, and people.&lt;/p&gt;
&lt;p&gt;This gives us a central denormalized location where the crucial measures are stored so that we don't have to join a
bunch of system tables together to derive those measures and some lookup tables that we can easily join to segment the
data depending on the task at hand.&lt;/p&gt;
&lt;p&gt;Take the following normalized set of tables from the &lt;a class="reference external" href="be-normal.html"&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="Complex OLTP diagram" src="/static/post4/post4_complexdiagram.jpg" style="width: 90%;" /&gt;
&lt;p&gt;Some of the business questions we may want to ask about these data are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;How much sales revenue was brought in over some time period by product category?&lt;/li&gt;
&lt;li&gt;How many orders were processed by store?&lt;/li&gt;
&lt;li&gt;How many units were shipped by location?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The facts here are most likely to be units, orders, and dollar revenue and we should be able to segment these by the
different groups of attributes shown - sales staff, stores, customer locations, and product categories. Here is
one option for a star schema to capture these requirements.&lt;/p&gt;
&lt;img alt="Sample star schema" src="/static/post5/post5_star.jpg" style="width: 100%;" /&gt;
&lt;p&gt;We have pre-aggregated the measures we care about and stored them with references to all of the descriptive attributes
to segment by in one fact_orders table. The dimensions are other denormalized tables that combine all characteristics
of the specific subjects in one table per subject. This simplifies the OLTP model substantially and makes it a lot
easier to answer the business questions above.&lt;/p&gt;
&lt;p&gt;For example, here is the SQL to answer how much sales revenue was brought in over some time period by product category.
${PERIOD_START} and ${PERIOD_END} are user inputs for the time period in question.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
        &lt;span class="n"&gt;FO&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ORDER_DATE&lt;/span&gt;
        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DP&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PRODUCT_CATEGORY&lt;/span&gt;
        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;SUM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FO&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ORDER_AMOUNT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;TOTAL_ORDER_AMOUNT&lt;/span&gt;

&lt;span class="k"&gt;FROM&lt;/span&gt;    &lt;span class="n"&gt;FACT_ORDERS&lt;/span&gt; &lt;span class="n"&gt;FO&lt;/span&gt;

&lt;span class="k"&gt;JOIN&lt;/span&gt;    &lt;span class="n"&gt;DIM_PRODUCT&lt;/span&gt; &lt;span class="n"&gt;DP&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;FO&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PRODUCT_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DP&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PRODUCT_ID&lt;/span&gt;

&lt;span class="k"&gt;WHERE&lt;/span&gt;   &lt;span class="n"&gt;FO&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ORDER_DATE&lt;/span&gt; &lt;span class="k"&gt;BETWEEN&lt;/span&gt; &lt;span class="err"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PERIOD_START&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="err"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PERIOD_END&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
        &lt;span class="n"&gt;FO&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ORDER_DATE&lt;/span&gt;
        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DP&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PRODUCT_CATEGORY&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Compare that to the logic required to get the same answer but from the normalized OLTP data model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
        &lt;span class="n"&gt;SOH&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderDate&lt;/span&gt; &lt;span class="n"&gt;ORDER_DATE&lt;/span&gt;
        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Name&lt;/span&gt; &lt;span class="n"&gt;PRODUCT_CATEGORY&lt;/span&gt;
        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;SUM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SOD&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderQty&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;SOD&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;UnitPrice&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;TOTAL_ORDER_AMOUNT&lt;/span&gt;

&lt;span class="k"&gt;FROM&lt;/span&gt;    &lt;span class="n"&gt;SalesOrderDetail&lt;/span&gt; &lt;span class="n"&gt;SOD&lt;/span&gt;

&lt;span class="k"&gt;JOIN&lt;/span&gt;    &lt;span class="n"&gt;SalesOrderHeader&lt;/span&gt; &lt;span class="n"&gt;SOH&lt;/span&gt;  &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;SOD&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SalesOrderID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SOH&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SalesOrderID&lt;/span&gt;
&lt;span class="k"&gt;JOIN&lt;/span&gt;    &lt;span class="n"&gt;Product&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;SOD&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ProductID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ProductID&lt;/span&gt;
&lt;span class="k"&gt;JOIN&lt;/span&gt;    &lt;span class="n"&gt;ProductSubcategory&lt;/span&gt; &lt;span class="n"&gt;PSC&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ProductSubcategoryID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PSC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ProductSubcategoryID&lt;/span&gt;
&lt;span class="k"&gt;JOIN&lt;/span&gt;    &lt;span class="n"&gt;ProductCategory&lt;/span&gt; &lt;span class="n"&gt;PC&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;PSC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ProductCategoryID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ProductCategoryID&lt;/span&gt;

&lt;span class="k"&gt;WHERE&lt;/span&gt;   &lt;span class="n"&gt;SOH&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderDate&lt;/span&gt; &lt;span class="k"&gt;BETWEEN&lt;/span&gt; &lt;span class="err"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PERIOD_START&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="err"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PERIOD_END&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
        &lt;span class="n"&gt;SOH&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderDate&lt;/span&gt;
        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Name&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's a lot of joins!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="snowflake-schemas"&gt;
&lt;h2&gt;Snowflake Schemas&lt;/h2&gt;
&lt;p&gt;The premise of snowflake schemas is essentially the same as star schemas but instead of denormalizing dimensions into
as few tables per subject as possible we instead normalize the dimensions. This is done to optimize storage space by
avoiding redundant information and make updates more efficient.&lt;/p&gt;
&lt;p&gt;In some cases your facts and dimensions may share many-to-many relationships. Storing the same attributes repeatedly for
each fact_key and dim_key combination can take up a lot of unnecessary space. It's preferred in these cases to create a
separate table that just stores the relationship between fact_key and dim_key without any additional attributes to use as
a bridge table from the fact to the dim. Other times you may not need some dimension attributes as often as others
so you may choose to store them separately in a different table so that queries are more efficient (in row-based databases).&lt;/p&gt;
&lt;p&gt;There's really no rule about how much normalization there should be. It's often dictated by the systems the data are
sourced from, the ETL processes in place, personal/team preference, what other data are stored alongside the schemas,
where the data are stored in, etc. What is fairly constant though is that snowflake schemas will most often lead to
worse query performance due to the extra joins that need to happen between fact and dim tables to answer business
questions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="are-these-modeling-techniques-used-in-practice"&gt;
&lt;h2&gt;Are These Modeling Techniques Used In Practice?&lt;/h2&gt;
&lt;p&gt;It depends. I have found that interviewers for data engineering positions focus a lot on star schema modeling but I have not
actually come across true star schemas that often on the job. I have seen relics of star schemas with dim tables here and
there but it always seems like people just abandon the approach and data tend to merge together into a set of core tables
combining both dims and facts.&lt;/p&gt;
&lt;p&gt;Part of the reason why is that nowadays companies are moving away from traditional data warehousing techniques and
storing the data in key-value/object stores like AWS S3. With tools like Spark on EMR and Presto/Athena, data doesn't
really need to be stored in any data warehouse for analysts to derive value from it. They can also take on different
types of structure where the data are completely denormalized into single datasets or split together in ways that don't
really adhere to any sort of schema. The types of tools used to consume the data offer a lot more flexibility in how
the data are read and manipulated than traditional data warehousing solutions.&lt;/p&gt;
&lt;p&gt;When data are stored in data warehouses like Redshift, they're typically stored in single tables that combine both facts
and dimensions. This is mainly because with efficient compute behind the scenes (either Redshift or Spark on data stored
in S3), it's easy to recompute large amounts of data (up to hundreds of terabytes) in a relatively short period of time.
That is if a dimension changes, the dataset owners can easily backfill that dimension into the one table that also has
facts within a matter of hours or days.&lt;/p&gt;
&lt;p&gt;I think these techniques are still worth understanding as logical data models, but they're not requirements
when physically storing the data.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Data Modeling"></category><category term="Data Modeling"></category></entry><entry><title>Be Normal</title><link href="https://decipheringbigdata.net/be-normal.html" rel="alternate"></link><published>2020-12-12T00:00:00-08:00</published><updated>2020-12-12T00:00:00-08:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.net,2020-12-12:/be-normal.html</id><summary type="html">&lt;p class="first last"&gt;The normal forms of OLTP data modeling&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="what-is-data-normalization"&gt;
&lt;h2&gt;What is Data Normalization?&lt;/h2&gt;
&lt;p&gt;Normalization is a fancy term to describe the process of organizing data into relations or tables to remove redundancy
through decomposition.&lt;/p&gt;
&lt;p&gt;There are 5 different forms of normalization:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;First Normal Form&lt;/li&gt;
&lt;li&gt;Second Normal Form&lt;/li&gt;
&lt;li&gt;Third Normal Form&lt;/li&gt;
&lt;li&gt;Boyce and Codd Normal Form&lt;/li&gt;
&lt;li&gt;Fourth Normal Form&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we go down this list data becomes less redundant and the more tables we end up with in our database. I'll walk
through concrete examples of these different forms below!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-world-without-normalization"&gt;
&lt;h2&gt;A World Without Normalization&lt;/h2&gt;
&lt;p&gt;First let's take a look at what lack of data normalization means. It helps to picture a giant spreadsheet that collects
all kinds of data about some subject. Let's say someone in your neighborhood keeps a spreadsheet of every single person
that lives in the neighborhood. It contains the relationships between neighbors in separate columns, the colors of the
houses, how many cars each house has, when deliveries and public services like trash collection pass by each house, etc.&lt;/p&gt;
&lt;p&gt;It looks something like this (shout out to &lt;a class="reference external" href="https://www.behindthename.com/random/"&gt;https://www.behindthename.com/random/&lt;/a&gt;).&lt;/p&gt;
&lt;img alt="Creepy neighbordhood spreadsheet" src="/static/post4/post4_normalization1.jpg" style="width: 100%;" /&gt;
&lt;p&gt;If keeping a creepy list did not raise any red flags before, this atrocity of a data structure should. It's not difficult to
imagine how difficult it would be to maintain data in such a list as it grows. Updates would be a pain! For example,
if a neighbor paints their house you would need to update each household member's record in this list, not just
a single house record. If Alf has a baby, you will need to add a family_3 column to record the relationship between the
existing 3 members and the new addition to the family. This affects all records in the table. Same thing if someone gets
a new car or the garbage collection routes change.&lt;/p&gt;
&lt;p&gt;So let's say your creepy neighbor wants to do better. What can they do?&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="first-normal-form"&gt;
&lt;h2&gt;First Normal Form&lt;/h2&gt;
&lt;p&gt;First normal form refers to data models that have only atomic values in each column and where no table has repeating
groups. Atomic values are simply those that can't be broken down into many values. Fortunately all of the values in
this spreadsheet are atomic. If we had a column called cars and in that column we had a record like
Toyota Camry, Toyota Camry, Toyota Corolla then this value would need to be broken down so that each value is stored
in its own record.&lt;/p&gt;
&lt;p&gt;What we do have here though are repeating groups. This refers to groups of values that can repeat for any one of the
primary keys in the tables. In this case, values that can repeat for each neighbor stored in the spreadsheet. Those
groups are the car, family, and friend columns.&lt;/p&gt;
&lt;p&gt;The solution to get rid of these repeating groups is to split the one table into 3 individual tables - neighbors,
vehicles, and relationships.&lt;/p&gt;
&lt;img alt="First normal form example" src="/static/post4/post4_fnf.jpg" style="width: 100%;" /&gt;
&lt;p&gt;The vehicles table will have the name of each person and a vehicle in their household. It's easier to update the vehicles
in each household now because we don't need to amend all records in the table by changing a column. We can simply amend,
delete, or add rows in the vehicles table. The same is true for relationships between neighbors. To add new ones we can
simply append rows to the relationships table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="second-normal-form"&gt;
&lt;h2&gt;Second Normal Form&lt;/h2&gt;
&lt;p&gt;Second normal form dictates that &amp;quot;all non-key attributes should be functionality dependent on the primary key.&amp;quot; What
this means in plain English is that each table should contain only information about one topic and all attributes in
that table should serve to describe the topic and nothing else.&lt;/p&gt;
&lt;p&gt;For example, in our first normal form model, the color of the house and the garbage routes are stored with the neighbors
table. The primary key of that table is the neighbor's name. Neither the house color nor the garbage route depend on
each neighbor. They instead depend solely on the house where the neighbors live in. Each house is uniquely identified
by an address in our data so each of these attributes should be stored in its own table as shown below.&lt;/p&gt;
&lt;img alt="Second normal form example" src="/static/post4/post4_snf.jpg" style="width: 100%;" /&gt;
&lt;p&gt;The advantage of this data model is that we remove the additional redundancy of having attributes related to the address
in the neighbors table where addresses can be repeated for each neighbor that lives in the same house as other neighbors.
For example, if we wanted to update the color of the house at 12234 NE 20th ST, we would only need to do it once in the
location_attributes table instead of 3 times in the neighbors table.&lt;/p&gt;
&lt;p&gt;It also means that if we delete any records from the neighbors table because people move out of the neighborhood, we
will still preserve all of the information related to the houses at the locations where they used to live.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="third-normal-form"&gt;
&lt;h2&gt;Third Normal Form&lt;/h2&gt;
&lt;p&gt;Tables should contain columns that are non-transitively dependent on the primary key.&lt;/p&gt;
&lt;img alt="Confused Marky Mark" src="https://media.giphy.com/media/zjQrmdlR9ZCM/giphy.gif" style="width: 60%;" /&gt;
&lt;p&gt;This one sounds complicated but it actually just means that we shouldn't store columns that depend on the primary key
of a table AND on other columns in that table. For example, our garbage_collection table has a column for the route
number and for the day when the garbage truck swings by at an address. Garbage route depends on the address and the day
of collection depends on the garbage route so a transitive dependency exists.&lt;/p&gt;
&lt;p&gt;The reason why we don't want these types of dependencies in our tables is because updates have to change multiple
attributes in a table when one attribute in the transitive dependency is updated, which can lead to inconsistencies.
For example, if we assign a different route to an address and that route runs on a different date then we also need to
update the date of collection for the address. We need to make sure to update both or else our data will be wrong.&lt;/p&gt;
&lt;p&gt;To fix it we can just add an additional table that stores the relationship between route and collection day.&lt;/p&gt;
&lt;img alt="Third normal form example" src="/static/post4/post4_tnf.jpg" style="width: 100%;" /&gt;
&lt;/div&gt;
&lt;div class="section" id="boyce-and-codd-normal-form"&gt;
&lt;h2&gt;Boyce and Codd Normal Form&lt;/h2&gt;
&lt;p&gt;This normal form adds a minor restriction to the third normal form - attributes should depend only on a super key (a
column or collection of columns that uniquely identify records in a table).&lt;/p&gt;
&lt;p&gt;Our data model above is both in 3NF and BCNF but suppose instead that we also stored the garbage collection crew number
in the garbage_routes table. The individual crew would determine the collection_day based on when they work in the week
so collection_day would depend on the crew number. However, crew number would not be a super key because one crew can
service multiple routes (crew number would not uniquely identify records in this table). This scenario would satisfy
3NF constraints but not BCNF constraints.&lt;/p&gt;
&lt;p&gt;We could fix a scenario like this by splitting the garbage_routes table into two, one storing the relationship between
route and crew and another storing the relationship between crew and collection_day.&lt;/p&gt;
&lt;img alt="BCNF example" src="/static/post4/post4_bcnf.jpg" style="width: 100%;" /&gt;
&lt;/div&gt;
&lt;div class="section" id="fourth-normal-form"&gt;
&lt;h2&gt;Fourth Normal Form&lt;/h2&gt;
&lt;p&gt;Finally, the fourth normal form requires us to avoid multi-valued dependencies in tables. This means that for any
dependency A -&amp;gt; B in a table, if multiple values of B exist for any single value of A and there are more than 2 columns
in that table then there is a multi-valued dependency violating the 4NF.&lt;/p&gt;
&lt;p&gt;Our BCNF data model above also satisfies 4NF but what if a single crew had multiple collection days and we also stored
the truck_id of each crew in the crew_collection_days table. Truck_id and collection_day here are independent of each
other so BCNF is satisfied but this would be a multi-valued dependency because the key crew_number can have multiple
collection days and can drive one or more trucks&lt;/p&gt;
&lt;p&gt;We can further normalize this by splitting crew_collection_days into two tables, one that maintain the one to many
relationship between crew_number and collection_day and another the one to many relationship between crew_number and
truck_id.&lt;/p&gt;
&lt;img alt="Fourth normal form example" src="/static/post4/post4_4nf.jpg" style="width: 100%;" /&gt;
&lt;/div&gt;
&lt;div class="section" id="normalize-all-the-tables"&gt;
&lt;h2&gt;Normalize All The Tables?&lt;/h2&gt;
&lt;p&gt;Normalization kicks ass, right? Well, not always. There are cases where we may want to do the opposite of normalizing
or as they say in the biz, &amp;quot;de-normalize.&amp;quot;&lt;/p&gt;
&lt;p&gt;Normalization works well in OLTP databases where tables are strongly tied to engineering systems that update them. These
are your point-of-sale, online checkout, messaging applications, etc. which are organized into individual
objects that maintain state and functionality for very specific components of the broader systems. The individual
objects may not be aware of other objects' state and so can only update the data for the specific table that backs
the one component. For example, a post class that's part of a forum web application updating a post table which just
contains information about individual posts on the forum and nothing else.&lt;/p&gt;
&lt;p&gt;For OLAP workloads that seek to answer overarching business questions, normalized databases can actually be
a hindrance. This is because to answer the types of analytical questions typically asked in these settings, an analyst
would need to first understand how all of the tables in a complex model like the one shown below fit together and then
write a massive query to join all of the tables together. Such a query would be inefficient and error-prone.&lt;/p&gt;
&lt;img alt="Complex OLTP diagram" src="/static/post4/post4_complexdiagram.jpg" style="width: 90%;" /&gt;
&lt;p&gt;For example, say an analyst were asked to produce a summary of total quantity ordered for product categories that were
under special offer during some time range. The analyst would need to join the SalesOrderHeader, SalesOrderDetail,
SpecialOfferProduct, SpecialOffer, Product, ProductSubcategory, and ProductCategory tables together to produce an
answer. Not a fun exercise!&lt;/p&gt;
&lt;p&gt;In these cases, pre-joining tables together or &amp;quot;de-normalizing&amp;quot; makes sense. We're willing to break normalization rules
and introduce some redundancy to our data models in order to make analytical queries more efficient and make the lives
of our analytics customers easier. This is where star and snowflake schemas come in handy but that's a different topic
for another day!&lt;/p&gt;
&lt;/div&gt;
</content><category term="Data Modeling"></category><category term="Data Modeling"></category></entry><entry><title>Python Parallelism</title><link href="https://decipheringbigdata.net/python-parallelism.html" rel="alternate"></link><published>2020-12-10T00:00:00-08:00</published><updated>2020-12-10T00:00:00-08:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.net,2020-12-10:/python-parallelism.html</id><summary type="html">&lt;p class="first last"&gt;Parallel processing with Python...or lack thereof?&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="something-about-the-gil"&gt;
&lt;h2&gt;Something About the GIL&lt;/h2&gt;
&lt;p&gt;The global interpreter lock. A mutex on Python objects that prevents multiple threads from executing at the same time.
Python is single-threaded. The end, right?&lt;/p&gt;
&lt;p&gt;Yes and no. It's a bit more complicated than that. The real answer is that you can get some pretty sweet parallelism
going if you know which libraries to use.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-threading-library"&gt;
&lt;h2&gt;The Threading Library&lt;/h2&gt;
&lt;p&gt;The first tool you should be aware of is the beautifully simple threading library. Say you have a function like the one
below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
    &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This finds the max element in a list of numbers but there is a delay of 10 seconds where the processor is simply waiting
for this function to finish and nothing is really being done. Let's run this function in separate threads and see how it
performs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;threading&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Thread&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;start_threads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;thread&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;thread&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wait_for_threads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;thread&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;thread&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Create 10 lists of 10 random numbers&lt;/span&gt;
    &lt;span class="n"&gt;list_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
    &lt;span class="n"&gt;lists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_count&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_count&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="c1"&gt;# Process each list in a separate thread&lt;/span&gt;
    &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Thread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;lists&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# Print time before execution&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="c1"&gt;# Start the threads and wait for them to finish&lt;/span&gt;
    &lt;span class="n"&gt;start_threads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;wait_for_threads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Print the time after execution&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you run this you'll see that the individual threads appear to run in parallel since all of the max values are
printed almost at the same time. Also, the amount of time that elapses from the start of the program to its end is
only slightly more than the 10 seconds the find_max function waits for before returning.&lt;/p&gt;
&lt;img alt="Running threading function results in parallel execution when process is I/O bound" src="/static/post3/post3_threading.jpg" style="width: 80%;" /&gt;
&lt;p&gt;However, this is not actually the case. Because of the GIL, there's really just one thread running! The magic though is
that the thread is able to switch between function executions when one is stuck waiting for an I/O bound process to
finish.&lt;/p&gt;
&lt;p&gt;In this case the first function execution is kicked off and immediately goes into the time.sleep(10). While it's waiting,
the CPU doesn't just sit around but runs the function for the second execution, which also waits for 10 seconds,
and so on until the 10 lists are processed in the 10 separate &amp;quot;threads&amp;quot; almost simultaneously.&lt;/p&gt;
&lt;p&gt;This functionality really shines when your process is stuck waiting for a response from a server, reading/writing a file
to disk, etc. Anything that is I/O bound where the CPU is not doing anything but just waiting. On the other hand, when
your CPU is doing intensive computations, the threading module is pretty much useless and can actually slow your program
down.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-multiprocessing-library"&gt;
&lt;h2&gt;The Multiprocessing Library&lt;/h2&gt;
&lt;p&gt;This library couldn't care less about the GIL. It just doesn't need to. What it does instead is execute your program in
separate Python interpreters within the same machine. The separate processes spawned will not share any memory space
or resources.&lt;/p&gt;
&lt;p&gt;The obvious drawback to this is the overhead with having multiple Python interpreters and separate memory spaces. That
said though, it provides an actual method to achieve true parallelism in Python and speed CPU-bound tasks considerably
on multi-core CPU systems.&lt;/p&gt;
&lt;p&gt;To test this out, let's modify our original function to not sleep for 10 seconds and instead we'll just run it on a
large amount of data to simulate a CPU-bound task.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;my_big_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000000&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_big_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This takes about 25 seconds to run on my machine.&lt;/p&gt;
&lt;img alt="Single CPU-bound process takes 25 seconds to run" src="/static/post3/post3_multiprocessing1.jpg" style="width: 80%;" /&gt;
&lt;p&gt;Below we perform the same task three times in separate threads using the threading library.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_big_list&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;my_big_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000000&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="n"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_big_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Run the function in separate threads&lt;/span&gt;
    &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Thread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;generate_big_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;thread&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;thread&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On my machine this took more than a minute to run! This makes sense because we have changed the find_max function to be
a CPU-bound process and threading just doesn't want to be our friend here.&lt;/p&gt;
&lt;img alt="Running threading function results in bad performance when process is CPU-bound" src="/static/post3/post3_multiprocessing2.jpg" style="width: 80%;" /&gt;
&lt;p&gt;So let's use the multiprocessing library to speed this bad boy up!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;max_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elm&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_num&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_big_list&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;my_big_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000000&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;find_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_big_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Run the function in separate processes&lt;/span&gt;
    &lt;span class="n"&gt;processes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;processes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;multiprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;generate_big_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;processes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;process&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;processes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;process&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By using multiprocessing we can run the three executions in 35 seconds, which is only 10 seconds more than the one
execution took to run.&lt;/p&gt;
&lt;img alt="Multiprocessing speeds up CPU-bound processes significantly" src="/static/post3/post3_multiprocessing3.jpg" style="width: 80%;" /&gt;
&lt;p&gt;As mentioned before, this performance comes at a price...three Python interpreters taking up three times the memory. One
for each process run by the multiprocessing library.&lt;/p&gt;
&lt;img alt="Running multiprocessing takes up a lot of overhead" src="/static/post3/post3_multiprocessing4.jpg" style="width: 80%;" /&gt;
&lt;/div&gt;
&lt;div class="section" id="are-there-other-options"&gt;
&lt;h2&gt;Are There Other Options?&lt;/h2&gt;
&lt;p&gt;Yes, the first option is not using any of these at all and just embracing the additional latency. There is beauty in
simplicity and sometimes an easy-to-follow single-threaded solution is preferable to the more efficient alternative.&lt;/p&gt;
&lt;p&gt;The second option to consider is a higher level API that combines threading and multiprocessing in one module.
Concurrent.futures provides a simpler interface but pretty much does the same thing as these two libraries. You can
read more about it &lt;a class="reference external" href="https://docs.python.org/3/library/concurrent.futures.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then there's &lt;a class="reference external" href="https://docs.python.org/3/library/asyncio.html"&gt;asyncio&lt;/a&gt;...that's a completely different beast best left
for a different time.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="Python"></category></entry><entry><title>Pelicans On AWS</title><link href="https://decipheringbigdata.net/pelicans-on-aws.html" rel="alternate"></link><published>2020-12-02T00:00:00-08:00</published><updated>2020-12-02T00:00:00-08:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.net,2020-12-02:/pelicans-on-aws.html</id><summary type="html">&lt;p class="first last"&gt;How I develop and host this site&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="huh"&gt;
&lt;h2&gt;Huh?&lt;/h2&gt;
&lt;p&gt;This is a post about how I created this site (so meta). I used a Python framework called Pelican for developing static
websites and hosted it behind a CloudFront CDN on S3. I don't think I've ever actually had fun deploying any web
anything until now so I figured it's a good topic to discuss!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="architecture"&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;In case you're not familiar with the phrase, a static website is simply a site that consists of files that are served
directly to the users of the site. There is no need to make request to backend servers for any sort of data like you
have with dynamic websites. Sites that lend themselves to static architectures are simple blogs, portfolios, small
business sites, etc. that don't require the user to interact with any service or dynamic functionality other than what
can be done with simple JS on the client-side. This makes things a lot easier because all you really need to do is write
the HTML/CSS and save it somewhere that's accessible from the Internet and voila you have a website.&lt;/p&gt;
&lt;p&gt;An obvious location to save these static files is &lt;a class="reference external" href="https://aws.amazon.com/s3/"&gt;S3&lt;/a&gt;. You can store anything you want
for cheap and use what you store with all other AWS services that enable you to deploy highly scalable websites without
writing complex code and managing servers yourselves. I found it easy to quickly set up a DNS name, load balancer,
SSL certificate, and CDN without worrying about all the complexities. This architecture is diagrammed below:&lt;/p&gt;
&lt;img alt="https://aws.amazon.com/blogs/security/how-to-protect-your-web-application-against-ddos-attacks-by-using-amazon-route-53-and-a-content-delivery-network/" src="/static/post2/post2_architecture.jpeg" style="width: 80%;" /&gt;
&lt;p&gt;Client requests for the different domain records associated with decipheringbigdata.net in the hosted zone get routed
to the CDN DNS for the different files available in S3. The files in S3 are only exposed via the CDN (CloudFront), which
efficiently delivers them to clients depending on where they are in the World. All I need to do is deploy the site's
files to the S3 bucket behind the CDN and they will automatically be served to users that visit decipheringbigdata.net.&lt;/p&gt;
&lt;p&gt;The site's HTTPS certificate was generated by &lt;a class="reference external" href="https://aws.amazon.com/certificate-manager/"&gt;Certificate Manager&lt;/a&gt; and
associated with the decipheringbigdata.net domain which was registered with &lt;a class="reference external" href="https://aws.amazon.com/route53/"&gt;Route 53&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below is the CloudFormation template that deploys this architecture to AWS.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;AWSTemplateFormatVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;2010-09-09&lt;/span&gt;
&lt;span class="nt"&gt;Description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;S3 / Route53 / CloudFront CloudFormation configuration&lt;/span&gt;

&lt;span class="nt"&gt;Parameters&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="c1"&gt;# SSL certificate created by CertificateManager&lt;/span&gt;
  &lt;span class="nt"&gt;AwsCertificateArn&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;         &lt;span class="l l-Scalar l-Scalar-Plain"&gt;String&lt;/span&gt;
    &lt;span class="nt"&gt;Default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;arn:aws:acm:us-east-1:146066720211:certificate/c681b862-cbdc-4d52-9029-13cc15a4c87f&lt;/span&gt;
    &lt;span class="nt"&gt;Description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Certificate must be created before CloudFormation stack so the value is fixed&lt;/span&gt;
  &lt;span class="nt"&gt;AwsRoute53CloudFrontHostedZoneId&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;         &lt;span class="l l-Scalar l-Scalar-Plain"&gt;String&lt;/span&gt;
    &lt;span class="nt"&gt;Default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Z2FDTNDATAQYW2&lt;/span&gt;
    &lt;span class="nt"&gt;Description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;CloudFront resources HostedZoneId&lt;/span&gt;
  &lt;span class="c1"&gt;# decipheringbigdata.net&lt;/span&gt;
  &lt;span class="nt"&gt;RootDomainName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Domain name for your website (example.com)&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;String&lt;/span&gt;

&lt;span class="nt"&gt;Resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="c1"&gt;# Identity to use in CDN&lt;/span&gt;
  &lt;span class="nt"&gt;DataBlogCloudFrontIdentity&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;AWS::CloudFront::CloudFrontOriginAccessIdentity&lt;/span&gt;
    &lt;span class="nt"&gt;Properties&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="nt"&gt;CloudFrontOriginAccessIdentityConfig&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nt"&gt;Comment&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Decipheringbigdata&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;Origin&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;Access&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;Identity&amp;quot;&lt;/span&gt;

  &lt;span class="c1"&gt;# Where to store the actual website files&lt;/span&gt;
  &lt;span class="nt"&gt;DataBlogS3Bucket&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;AWS::S3::Bucket&lt;/span&gt;
    &lt;span class="nt"&gt;Properties&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="nt"&gt;BucketName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;RootDomainName&lt;/span&gt;

  &lt;span class="c1"&gt;# CDN distribution of the files in the S3 bucket above&lt;/span&gt;
  &lt;span class="nt"&gt;DataBlogCloudFront&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;AWS::CloudFront::Distribution&amp;quot;&lt;/span&gt;
    &lt;span class="nt"&gt;Properties&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="nt"&gt;DistributionConfig&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nt"&gt;Aliases&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;RootDomainName&lt;/span&gt;
        &lt;span class="nt"&gt;Comment&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;RootDomainName&lt;/span&gt;
        &lt;span class="nt"&gt;DefaultCacheBehavior&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="nt"&gt;AllowedMethods&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;GET&lt;/span&gt;
            &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;HEAD&lt;/span&gt;
          &lt;span class="nt"&gt;CachedMethods&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;GET&lt;/span&gt;
            &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;HEAD&lt;/span&gt;
          &lt;span class="nt"&gt;ForwardedValues&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="nt"&gt;QueryString&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;True&lt;/span&gt;
          &lt;span class="nt"&gt;TargetOriginId&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="kt"&gt;!Join&lt;/span&gt; &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt; &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;S3-origin-&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="nv"&gt;RootDomainName&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;
          &lt;span class="nt"&gt;ViewerProtocolPolicy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;redirect-to-https&lt;/span&gt;
        &lt;span class="nt"&gt;DefaultRootObject&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;index.html&lt;/span&gt;
        &lt;span class="nt"&gt;Enabled&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;True&lt;/span&gt;
        &lt;span class="nt"&gt;HttpVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;http2&lt;/span&gt;
        &lt;span class="nt"&gt;IPV6Enabled&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;True&lt;/span&gt;
        &lt;span class="nt"&gt;Origins&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;DomainName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!GetAtt&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;DataBlogS3Bucket.RegionalDomainName&lt;/span&gt;
            &lt;span class="nt"&gt;Id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Join&lt;/span&gt; &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt; &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;S3-origin-&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="nv"&gt;RootDomainName&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;
            &lt;span class="nt"&gt;S3OriginConfig&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
              &lt;span class="nt"&gt;OriginAccessIdentity&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Sub&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;origin-access-identity/cloudfront/${DataBlogCloudFrontIdentity}&amp;quot;&lt;/span&gt;
        &lt;span class="nt"&gt;PriceClass&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;PriceClass_All&lt;/span&gt;
        &lt;span class="nt"&gt;ViewerCertificate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="nt"&gt;AcmCertificateArn&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;AwsCertificateArn&lt;/span&gt;
          &lt;span class="nt"&gt;MinimumProtocolVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;TLSv1.2_2018&lt;/span&gt;
          &lt;span class="nt"&gt;SslSupportMethod&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;sni-only&lt;/span&gt;

  &lt;span class="c1"&gt;# Allow access only by the CDN identity; no public access to the S3 files themselves is allowed&lt;/span&gt;
  &lt;span class="nt"&gt;DataBlogS3BucketPolicy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;AWS::S3::BucketPolicy&lt;/span&gt;
    &lt;span class="nt"&gt;Properties&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="nt"&gt;Bucket&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;DataBlogS3Bucket&lt;/span&gt;
      &lt;span class="nt"&gt;PolicyDocument&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nt"&gt;Statement&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;Action&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
              &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;s3:GetObject&amp;quot;&lt;/span&gt;
            &lt;span class="nt"&gt;Effect&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Allow&lt;/span&gt;
            &lt;span class="nt"&gt;Principal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
              &lt;span class="nt"&gt;AWS&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="kt"&gt;!Join&lt;/span&gt; &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;
                  &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;
                  &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;
                    &lt;span class="s"&gt;&amp;quot;arn:aws:iam::cloudfront:user/CloudFront&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;Origin&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;Access&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;Identity&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;
                    &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="nv"&gt;DataBlogCloudFrontIdentity&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;
                  &lt;span class="p p-Indicator"&gt;],&lt;/span&gt;
                &lt;span class="p p-Indicator"&gt;]&lt;/span&gt;
            &lt;span class="nt"&gt;Resource&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Join&lt;/span&gt; &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt; &lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;arn:aws:s3:::&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="nv"&gt;DataBlogS3Bucket&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;/*&amp;quot;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;
        &lt;span class="nt"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;2012-10-17&amp;quot;&lt;/span&gt;

  &lt;span class="c1"&gt;# How to redirect requests to decipheringbigdata.net to the CDN&lt;/span&gt;
  &lt;span class="nt"&gt;DataBlogRoute53&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;AWS::Route53::RecordSet&lt;/span&gt;
    &lt;span class="nt"&gt;Properties&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="nt"&gt;AliasTarget&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nt"&gt;DNSName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!GetAtt&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;DataBlogCloudFront.DomainName&lt;/span&gt;
        &lt;span class="nt"&gt;EvaluateTargetHealth&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;False&lt;/span&gt;
        &lt;span class="nt"&gt;HostedZoneId&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;AwsRoute53CloudFrontHostedZoneId&lt;/span&gt;
      &lt;span class="nt"&gt;Comment&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;RootDomainName&lt;/span&gt;
      &lt;span class="nt"&gt;HostedZoneName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Sub&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;${RootDomainName}.&amp;#39;&lt;/span&gt;
      &lt;span class="nt"&gt;Name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;RootDomainName&lt;/span&gt;
      &lt;span class="nt"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;A&lt;/span&gt;

&lt;span class="nt"&gt;Outputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;Route53URL&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;        &lt;span class="kt"&gt;!Ref&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;DataBlogRoute53&lt;/span&gt;
    &lt;span class="nt"&gt;Description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="s"&gt;&amp;quot;DataBlog&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;URL&amp;quot;&lt;/span&gt;
  &lt;span class="nt"&gt;CloudFrontURL&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;Value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;        &lt;span class="kt"&gt;!GetAtt&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;DataBlogCloudFront.DomainName&lt;/span&gt;
    &lt;span class="nt"&gt;Description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="s"&gt;&amp;quot;DataBlogCloudFront&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;URL&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="the-pelican"&gt;
&lt;h2&gt;The Pelican&lt;/h2&gt;
&lt;p&gt;One of the advantages of dynamic sites is that you can use the web application frameworks to serve HTML in a reusable
format with the help of templating engines like Jinja. Frameworks like Node.js, Django, and Flask all have this ability
and make it easy to manage the different resources associated with sites in distributable software packages.
When I switched over to a static site implementation of this blog I went searching for a way to seamlessly make
formatting changes to all of the static files in S3 and deploy them without breaking a sweat like I was used to with
these types of dynamic web application frameworks.&lt;/p&gt;
&lt;p&gt;Alas, I came across this amazing Python framework for static website generation called &lt;a class="reference external" href="https://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;.
I'm in love with this thing! It makes producing content for static sites a breeze.&lt;/p&gt;
&lt;p&gt;The basic gist is that you can write your content in a predefined location with easy to read Markdown or reStructuredText
and provide your settings in a config file using native Python. Pelican will then wrap up your little precious site baby
and deliver it seamlessly to its destination with all of its necessary static, CSS, and HTML files. You can also plug in
various &lt;a class="reference external" href="http://www.pelicanthemes.com/"&gt;themes from open source contributions&lt;/a&gt; without having to mess with the CSS or
Bootstrap idioms in the HTML.&lt;/p&gt;
&lt;p&gt;To see it in action, you can set up a quickstart site yourself as documented &lt;a class="reference external" href="https://opensource.com/article/19/1/getting-started-pelican"&gt;here&lt;/a&gt;
and below:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Install Pelican in a Python virtual environment&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python3 -m venv venv
./venv/bin/pip install --upgrade pip
./venv/bin/pip install pelican
&lt;/pre&gt;&lt;/div&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;Create a Hello World site and skeleton code to get started&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;./venv/bin/pelican-quickstart
&lt;/pre&gt;&lt;/div&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;Fill out the form like I have done so below&lt;/li&gt;
&lt;/ol&gt;
&lt;img alt="Running ./venv/bin/pelican-quickstart for decipheringbigdata.net" src="/static/post2/post2_pelicansetup.jpeg" style="width: 80%;" /&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;Start a local dev server and preview the site at &lt;a class="reference external" href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt; venv/bin/activate
make devserver
&lt;/pre&gt;&lt;/div&gt;
&lt;ol class="arabic simple" start="5"&gt;
&lt;li&gt;Deploy to S3&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt; venv/bin/activate
make s3_upload
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To make additions to your site you can edit the different configuration in the pelicanconf.py file and add .rst or .md
content files to your content directory. As mentioned before you can also leverage a suite of themes built by the
open source community as documented in &lt;a class="reference external" href="https://github.com/getpelican/pelican-themes/"&gt;this&lt;/a&gt; Github repo. For
decipheringbigdata.net I have used the &lt;a class="reference external" href="https://github.com/gilsondev/pelican-clean-blog/"&gt;clean-blog theme&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you deploy to S3 using the Makefile that the quickstart setup provides with the basic site, Pelican will bundle
up the generated output directory and copy it to the S3 bucket you specify. This should be the bucket that you've set
up the AWS static site infrastructure with.&lt;/p&gt;
&lt;p&gt;That's it! End-to-end creation of static content and deployment to a scalable web infrastructure using Pelican and AWS.
If you're curious, you can checkout my &lt;a class="reference external" href="https://github.com/adaros92/data-blog/"&gt;Github repo for decipheringbigdata.com&lt;/a&gt;
to see the full code. Enjoy!&lt;/p&gt;
&lt;/div&gt;
</content><category term="Web Development"></category><category term="Web"></category></entry><entry><title>ML Demystified: Text Classification with Naive Bayes</title><link href="https://decipheringbigdata.net/ml-demystified-text-classification-with-naive-bayes.html" rel="alternate"></link><published>2020-12-01T19:54:00-08:00</published><updated>2020-12-01T19:54:00-08:00</updated><author><name>Adams Rosales</name></author><id>tag:decipheringbigdata.net,2020-12-01:/ml-demystified-text-classification-with-naive-bayes.html</id><summary type="html">&lt;p class="first last"&gt;Implementing Naive Bayes from scratch using Python&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="enter-bayes"&gt;
&lt;h2&gt;Enter Bayes&lt;/h2&gt;
&lt;p&gt;Say you have a collection of documents and some classes those documents belong to. You want to create some model to help
you label other documents with the most likely class based on these data. For example, you have data on thousands of
e-mails and whether those e-mails contain spam or not and want to use this information to filter future spam e-mails
from ever reaching your inbox. Let's code a simple Naive Bayes algorithm in Python to help us do that!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="first-some-math"&gt;
&lt;h2&gt;First Some Math&lt;/h2&gt;
&lt;p&gt;Naive Bayes is based on Bayes' Rule. This theorem provides a way to estimate the probability of some event A occurring
when another event B has occurred when we know the likelihood of B occurring when A has occurred and the probability of
A and B. The formula is P(A|B) = P(A) P(B|A)/P(B).&lt;/p&gt;
&lt;p&gt;As an example, say you have a standard deck of 52 cards.
What is the probability of drawing a queen if you draw a face card (queen, king, or jack)? Let's use Bayes' Rule.
The probability of drawing a queen given a face card = [(probability of drawing a queen) X (the probability of drawing a
face card given that you drew a queen)] / (the probability of drawing a face card). Well the probability of drawing a
face card given that you drew a queen is just 1 because queens are always considered a face card. The probability of
drawing a queen is 4/52 (4 queens in a deck) and the probability of drawing a face card is 12/52 (4 jacks, 4 queens,
and 4 kings in a deck) so ((4/52) X 1)/(12/52) = 4/12 or 1/3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="naive-bayes-algorithm-for-text-classification"&gt;
&lt;h2&gt;Naive Bayes Algorithm for Text Classification&lt;/h2&gt;
&lt;p&gt;Now that we've gotten that out of the way, let us dive into the algorithm with the spam filtering example!
Suppose you have the following condensed e-mail: &amp;quot;Dear sir, I am Dr. Tunde, brother of Nigerian Prince.&amp;quot; We want to
classify it as either spam or not spam. We can utilize Bayes' Rule here as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will simply calculate these conditional probabilities and choose the class (spam or not spam) that gives us the
highest one. Knowing this, we can remove the denominator since it's the same in both equations and we just want to
determine which yields the highest probability.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay so now we have a simple equation we can work with. Look at all your previous e-mails in your training data that
were classified as spam and not spam. Then count the number of times &amp;quot;Dear sir, I am Dr. Tunde, brother of Nigerian
Prince&amp;quot; appears in both sets of e-mails. Assuming an equal number of spam and not spam e-mails, the class with the
highest count of this phrase will result in the highest probability and this is the class we'll choose for this
particular e-mail.&lt;/p&gt;
&lt;p&gt;There is one major obstacle here. What if this phrase doesn't exist in our past e-mails data? We'll have a probability
of zero for both spam and not spam, rendering the algorithm useless. Well instead of looking at whole phrases, let's
consider the individual words that make up those phrases. We can rewrite the conditional probabilities as follows if we
assume that words in a phrase are independent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sir&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Prince&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dear sir, I am...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dear&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sir&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Prince&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Not&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now the algorithm shifts from counting whole phrases to counting individual words within the two types of e-mails in
the data and multiplying the individual word counts or probabilities. This puts the &amp;quot;Naive&amp;quot; in Naive Bayes because
we're making a big assumption that the words are independent of each other. That is what gives us the freedom to
multiply the individual word probabilities. The tradeoff is that words in a phrase are not actually independent of
each other. For example, the position of the word &amp;quot;and&amp;quot; depends on other words before and after it in a phrase.&lt;/p&gt;
&lt;p&gt;Back to the algorithm! All we need to do is calculate these probabilities. The probability of spam/not spam is simple
to determine. Just count the number of spam and not spam e-mails and divide by the total number of e-mails. Let's say
we have 100 spam e-mails and 200 not spam e-mails in our training data. The probability of spam is 1/3 and the
probability of not spam is 2/3. The individual word probabilities are a little more nuanced. We count the number of times
each word appears in the training data for each type of e-mail and divide by the total number of words in each type.
Suppose &amp;quot;Dear&amp;quot; appears 50 times in all spam e-mails and 25 times in non-spam e-mails and there are 500 words in total
across all spam e-mails and 600 across non-spam e-mails. The probability of &amp;quot;Dear&amp;quot; given spam is 50/500 or 1/10 and
given not spam is 25/600 or 1/24.&lt;/p&gt;
&lt;p&gt;Let's consider the case where the word &amp;quot;Dear&amp;quot; does not appear in any of the spam e-mails in the training data. The
probability of &amp;quot;Dear&amp;quot; would then be 0 and since we're multiplying probabilities, the total probability of all words
multiplied together would be 0. The way to get around this is to add 1 to each word probability numerator. Then to
ensure that we always have a number between 0 and 1 for our probabilities, we add a larger number to the denominator.&lt;/p&gt;
&lt;p&gt;Each individual word's probability is then [(number of times the word appears in a class of documents) + 1]/[(number of
words in the class of documents) + vocabulary]. The vocabulary value is just the number of unique words in all documents
of all classes in the training data. This procedure is termed additive or Laplace smoothing and it ensures that none of
the individual word probabilities can be 0 or greater than 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="implementing-the-algorithm"&gt;
&lt;h2&gt;Implementing the Algorithm&lt;/h2&gt;
&lt;p&gt;Now that we have a general understanding of how the Naive Bayes algorithm works, let's code it up in Python!&lt;/p&gt;
&lt;p&gt;Let's start by declaring the functions we will implement one by one.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parse_training_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_obj&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Given an object of document sets per class this function extracts the vocabulary&lt;/span&gt;
&lt;span class="sd"&gt;    across all documents, the count of unique words by class of document, and the count&lt;/span&gt;
&lt;span class="sd"&gt;    of document types. It returns a tuple like the one below.&lt;/span&gt;

&lt;span class="sd"&gt;    (&lt;/span&gt;
&lt;span class="sd"&gt;        count of unique words across all documents (number),&lt;/span&gt;
&lt;span class="sd"&gt;        count of words by class (dictionary - {cls_name:{word:count, total:total words}}),&lt;/span&gt;
&lt;span class="sd"&gt;        count of documents of each class (dictionary - {cls_name:count, total:total_docs})&lt;/span&gt;
&lt;span class="sd"&gt;    )&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_probability&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_counter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_documents&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; This function takes in a dictionary of word counts , the total&lt;/span&gt;
&lt;span class="sd"&gt;    vocabulary across all documents in the training data, the of documents by class,&lt;/span&gt;
&lt;span class="sd"&gt;    and documents we want to classify and calculates the probability of the test&lt;/span&gt;
&lt;span class="sd"&gt;    documents to belonging each class. It returns a list of dictionaries containing&lt;/span&gt;
&lt;span class="sd"&gt;    the class of document as the key and the probability of belonging to that&lt;/span&gt;
&lt;span class="sd"&gt;    class as the value.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parsed_train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; This function takes in the parsed training data object from parse_training_data&lt;/span&gt;
&lt;span class="sd"&gt;    above and the test data as a list of documents that we want to classify.&lt;/span&gt;
&lt;span class="sd"&gt;    It then calls calculate_probability, which returns all the probabilities of belonging&lt;/span&gt;
&lt;span class="sd"&gt;    to each class. It iterates over this list which will have one dictionary for each&lt;/span&gt;
&lt;span class="sd"&gt;    test document and it chooses the class of document that has the largest probability&lt;/span&gt;
&lt;span class="sd"&gt;    value. The function then returns a list of predicted document types for each test&lt;/span&gt;
&lt;span class="sd"&gt;    document.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;naive_bayes_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Takes in a training data object with the class of document as the key and a&lt;/span&gt;
&lt;span class="sd"&gt;    list of documents as the value. Calls parse_training_data bove and predict to label&lt;/span&gt;
&lt;span class="sd"&gt;    each document with a class based on the training data. Returns  a list of predicted&lt;/span&gt;
&lt;span class="sd"&gt;    classes corresponding to each document in the test data.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let's implement the parse_training_data function. This function takes a Python dictionary as input. The dictionary
will be structured just like our toy training data at the bottom of this page (the class of document as the key and a
list of documents as the value for each key). It will iterate over each key and value pair in this dictionary, iterate
over each document in the list of documents for each class, and over each word in each document. As it does this, it
will keep track of the unique words it encounters in the set called vocab_set, maintain a class counter dictionary which
has the count of unique documents by type, and a word counter dictionary which has the count by word for each class. It
will then return a tuple with the length of the vocab_set (unique words across all documents which acts as our
vocabulary), the word counts by class of document, and the count of unique documents by class of document.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parse_training_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_obj&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Given an object of document sets per class this function extracts the vocabulary&lt;/span&gt;
&lt;span class="sd"&gt;    across all documents, the count of unique words by class of document, and the count&lt;/span&gt;
&lt;span class="sd"&gt;    of document types. It returns a tuple like the one below.&lt;/span&gt;
&lt;span class="sd"&gt;    (&lt;/span&gt;
&lt;span class="sd"&gt;        count of unique words across all documents (number),&lt;/span&gt;
&lt;span class="sd"&gt;        count of words by class (dictionary - {cls_name:{word:count, total:total words}}),&lt;/span&gt;
&lt;span class="sd"&gt;        count of documents of each class (dictionary - {cls_name:count, total:total_docs})&lt;/span&gt;
&lt;span class="sd"&gt;    )&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;vocab_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;word_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;class_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;class_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="c1"&gt;# Break data object into class_name, documents&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;docs&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data_obj&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;class_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="n"&gt;word_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="c1"&gt;# Iterate over each document in list of documents&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Increment the count of document type&lt;/span&gt;
            &lt;span class="n"&gt;class_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;class_total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="c1"&gt;# Iterate over each word in the document&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word_orig&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="c1"&gt;# Convert all words to same case&lt;/span&gt;
                &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word_orig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="c1"&gt;# Add the word to the vocabulary set&lt;/span&gt;
                &lt;span class="n"&gt;vocab_set&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;word_total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="c1"&gt;# Increment the count of word in the current class&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="c1"&gt;# Add the count of total words in the class to word counts dict&lt;/span&gt;
        &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word_total&lt;/span&gt;
    &lt;span class="c1"&gt;# Add the count of total classes to the class counts dict&lt;/span&gt;
    &lt;span class="n"&gt;class_counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;class_total&lt;/span&gt;
    &lt;span class="c1"&gt;# Return (unique vocabulary, count of words by class, and count of docs by class)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab_set&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_counts&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next let's implement the calculate_probability function. Here we're going to take in a dictionary of word counts by
document class, a vocabulary number (count of unique words in the training data),  a dictionary of document counts by
document class, and a list of documents to calculate probabilities for. The probabilities will denote the likelihood of
belonging to each document class by test document. The function will return a list of dictionaries where each dictionary
corresponds to each document in the test document input list and contains key-value pairs with the class of document as
the key and the probability of belonging to that document as the value.&lt;/p&gt;
&lt;p&gt;As defined in the algorithm section above, the total probability of belonging to a certain class of document is
calculated by multiplying the individual word probabilities and the probability of the document class itself. However,
when actually implementing this calculation, we need to be able to handle very small floating point numbers. Since we're
multiplying probabilities between 0 and 1, we can get very small floats that may result in arithmetic underflow. This
happens when the result of a calculation is a number that is too small for the computer to store in memory. A solution
to this is to take the log of the individual word probabilities and add these log values (recall from math class long
ago that log(xy) = log(x) + log(y)). Let's do it!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_probability&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_counter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_documents&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; This function takes in a dictionary of word counts , the total&lt;/span&gt;
&lt;span class="sd"&gt;    vocabulary across all documents in the training data, the of documents by class,&lt;/span&gt;
&lt;span class="sd"&gt;    and documents we want to classify and calculates the probability of the test&lt;/span&gt;
&lt;span class="sd"&gt;    documents to belonging each class. It returns a list of dictionaries containing&lt;/span&gt;
&lt;span class="sd"&gt;    the class of document as the key and the probability of belonging to that&lt;/span&gt;
&lt;span class="sd"&gt;    class as the value.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;rslt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="c1"&gt;# Iterate over each document in the list of test documents we want to classify&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;test_documents&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="c1"&gt;# Iterate over each document class in word_counter&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc_cls&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_counts&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;word_counter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;# Skip if the class is total, which is the total count of words&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;doc_cls&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;total&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;continue&lt;/span&gt;
            &lt;span class="c1"&gt;# Start with probability of that class based on training data&lt;/span&gt;
            &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;doc_cls&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;doc_cls&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
                &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="c1"&gt;# Iterate over each word in the document&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word_orig&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word_orig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="c1"&gt;# Get word count or 0 if not found in training data&lt;/span&gt;
                &lt;span class="n"&gt;word_instance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;word_instance&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;word_instance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
                &lt;span class="c1"&gt;# Add in the log of (count of word + 1)/(total words + vocab) to probability&lt;/span&gt;
                &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;doc_cls&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;word_instance&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;word_counter&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="c1"&gt;# Add in the probability by document class to results list&lt;/span&gt;
        &lt;span class="n"&gt;rslt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rslt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next let's implement the predict function. This function is just tasked with taking the list of probabilities by class
that we get from calculate_probability and choosing the document class corresponding to the largest probability for
each document in the test list. The result of this function are the actual predicted values for each document we fed
the algorithm.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parsed_train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; This function takes in the parsed training data object from parse_training_data&lt;/span&gt;
&lt;span class="sd"&gt;    above and the test data as a list of documents that we want to classify.&lt;/span&gt;
&lt;span class="sd"&gt;    It then calls calculate_probability, which returns all the probabilities of belonging&lt;/span&gt;
&lt;span class="sd"&gt;    to each class. It iterates over this list which will have one dictionary for each&lt;/span&gt;
&lt;span class="sd"&gt;    test document and it chooses the class of document that has the largest probability&lt;/span&gt;
&lt;span class="sd"&gt;    value. The function then returns a list of predicted document types for each test&lt;/span&gt;
&lt;span class="sd"&gt;    document.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parsed_train_data&lt;/span&gt;
    &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_probability&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;rslt_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="c1"&gt;# Iterate over each doc class : probability dictionary&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prob_obj&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;max_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="n"&gt;max_cls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="c1"&gt;# Select the class from the probability dictionary that has the largest probability&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;cls_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prob_obj&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;max_prob&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;max_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;max_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;
                &lt;span class="n"&gt;max_cls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cls_name&lt;/span&gt;
        &lt;span class="c1"&gt;# Add the document class corresponding to the largest probability to the result&lt;/span&gt;
        &lt;span class="n"&gt;rslt_classes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_cls&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rslt_classes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally we implement the main interface function we can expose and call with our training and test data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;naive_bayes_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Takes in a training data object with the class of document as the key and a&lt;/span&gt;
&lt;span class="sd"&gt;    list of documents as the value. Calls parse_training_data bove and predict to label&lt;/span&gt;
&lt;span class="sd"&gt;    each document with a class based on the training data. Returns  a list of predicted&lt;/span&gt;
&lt;span class="sd"&gt;    classes corresponding to each document in the test data.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;parsed_obj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parse_training_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parsed_obj&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's call our naive_bayes_text function with some toy training and test data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="c1"&gt;# Create toy training data&lt;/span&gt;
    &lt;span class="n"&gt;training_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;spam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Dear sir, I am Dr Tunde, brother of Nigerian Prince&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Win a million dollars today&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;48 hours clearance ends now 48 hours 48 hours Free stuff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Private invite to exclusive event&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Discount inside 90 percent off everything&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;12 days of deals happening now Closeout sale Free giveaways and more&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;This is your last chance to register for the biggest giveaway of the year&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Your attention is needed for this very important message&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Tick-tock it&amp;#39;s the last day for 30 percent off your purchase&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Final hours Mega mega mega mega mega free shipping on all items&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Checkout these last minute deals on all electronics&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Dear sir, please join me in this one of a lifetime opportunity&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;not spam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;It was great catching up with you yesterday give me a call anytime&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Please remember to bring the drink ingredients to the party&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;How did your final exam go yesterday&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Please give me a call back&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Thanks for inquiring about transferring the non-IRA assets from your personal account&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;You have a package to pick up at the lobby hub&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;You have a package to pick up at the lobby hub&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Thanks for reaching out, a member of our team will get back to you&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;You have a package to pick up at the lobby hub&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;Payment successfully processed for account ending in&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;I am attaching mom&amp;#39;s favorite mulled wine recipe that you can use for this weekend&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;How are the kids doing&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="c1"&gt;# Train the naive bayes model and predict classes for some toy test data&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;naive_bayes_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;How did your final exam go&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Last minute clearance discount&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Nigerian Prince&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Payment for your kids processed successfully&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We get the following result for our document list ([&amp;quot;How did your final exam go&amp;quot;, &amp;quot;Last minute clearance discount&amp;quot;,
&amp;quot;Nigerian Prince&amp;quot;,&amp;quot;Payment for your kids processed successfully&amp;quot;]):&lt;/p&gt;
&lt;p&gt;['not spam', 'spam', 'spam', 'not spam']&lt;/p&gt;
&lt;p&gt;There you go. We have successfully implemented a basic Naive Bayes algorithm for text classification just by applying
some simple counting. The final code can be retrieved from my
&lt;a class="reference external" href="https://github.com/adaros92/ml_demystified/blob/master/naive_bayes.py"&gt;Github repository&lt;/a&gt; for this
series. Try experimenting with it and running it on real world data. You can change it up so that you don't have to
process the entire training data each time. You can also create the ability to feed the model new data and improve it
incrementally. Have fun!&lt;/p&gt;
&lt;/div&gt;
</content><category term="Machine Learning"></category><category term="ML"></category></entry></feed>