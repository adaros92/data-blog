<!DOCTYPE html>
<html lang="en">
<head>
	<link rel="stylesheet" type="text/css" href="https://decipheringbigdata.com/theme/css/style.css">
	<!--<link rel="stylesheet/less" type="text/css" href="/theme/css/style.less">-->
	<!--<script src="/theme/js/less.js" type="text/javascript"></script>-->
	<link rel="stylesheet" type="text/css" href="https://decipheringbigdata.com/theme/css/pygments.css">
	<link href='//fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>



		<title>Deciphering Big Data</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
            <a href="https://decipheringbigdata.com"><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
            <h1 id="user"><a href="https://decipheringbigdata.com" class="">Adams Rosales</a></h1>
			<h2></h2>
			<p class="bio">has some opinions about data engineering</p>
			<ul>
					<a href="https://decipheringbigdata.com/pages/about-me.html">About Me</a><br><br>
					<a href="https://www.linkedin.com/in/adamsr09/">LinkedIn</a>
			</ul>
		</div>
		<footer>
			<address>
				Powered by <a href="https://blog.getpelican.com/">Pelican</a>
			</address>
		</footer>
	</section>

	<section id="posts">
	<header>
		<h1>Adams Rosales's blog</h1>
		<h3>Posted May 22, 2022</h3>
	</header>
	<article>
		<h1 id="title">
			<a href="https://decipheringbigdata.com/what-to-learn-to-become-a-data-engineer.html" rel="bookmark"
				title="Permalink to What To Learn To Become a Data Engineer">What To Learn To Become a Data Engineer</a>
		</h1>
		<div class="section" id="what-even-is-a-data-engineer">
<h2>What even is a data engineer?</h2>
<p>It's not so straightforward! You see, similar to data scientist, the role of data engineer is quite new to the tech scene. There isn't as much uniformity in responsibilities as there is with software engineers, which is a role that has been around for much longer. There are some common skills that most data engineers out there share but what you actually end up doing and therefore need to be familiar with entirely depends on the company and in some cases, the team within that company.</p>
</div>
<div class="section" id="skills-to-learn">
<h2>Skills to learn</h2>
<div class="section" id="fundamental-skills">
<h3>Fundamental skills</h3>
<p>These are the skills that are common among most data engineers regardless of whether they're more on the analytical/product side or the engineering side of the spectrum.</p>
<ul class="simple">
<li><strong>SQL</strong> - working in data and not knowing SQL is a cardinal sin! Not knowing SQL as a data professional is like working as a professional musician, but not knowing how to read or write music. You could probably do it if you're good at playing your instrument, but it would seriously hinder you when working with other professional musicians. Simply put, SQL is the language of data. Even if you as a data engineer build pipelines and datasets with a programming language like Python or Scala, your peers over in analytics, finance, marketing, etc. will probably be using SQL to query the data. Not being able to work with them because you're not familiar with SQL is a problem you want to avoid.</li>
<li><strong>Data reasoning</strong> - you need to understand how data are used to inform decisions. What are the key metrics you can derive from raw data by curating it? What aggregations and segmenting do you need to apply to the data to answer business questions? How can the data be best represented? These are key questions that you should know the answer to as a data engineer because it's your north star. The whole point of collecting data and processing it is to make it usable for people. This may involve knowing a little math and statistics, being familiar with basic visualization techniques, and knowing how to tell a story with data, even if you're not directly doing those things.</li>
<li><strong>Data modeling</strong> - to make data usable and easy to work with for others, you need to model it. There are many approaches out there depending on how the data will be used. For example, the way you model data in a transactional system may be very different than in a purely analytical setting. You as a data engineer need to be aware of how the data will be consumed, which will then inform the degree of <a class="reference external" href="https://decipheringbigdata.com/be-normal.html">normalization</a> that needs to be applied and whether to leave your data in a very raw state or to curate it heavily.</li>
<li><strong>Pipeline orchestration</strong> - this is how you make the classic ETL (extract, transform, load) process happen. Every stage in your pipeline needs to be orchestrated, i.e. run by some system. Again, there are many ways and tools out there to do this. Airflow is a commonly used tool, but you may also find yourself running scripts on your own servers or using serverless solutions like <a class="reference external" href="https://decipheringbigdata.com/my-friend-sam.html">AWS Lambda</a> to run your workflows. Regardless of which tool you use, the principles are the same. You break your pipeline up into different stages and run them sequentially or out of order on a cadence or in real time, depending on how much data you need to ingest, how frequently it needs to be ingested, how much processing needs to be done, and where you need to load it.</li>
<li><strong>Data privacy</strong> - the privacy of customers from whom you collect data is paramount to anything else, but it's often a lower priority for engineers who just want to build cool solutions. Nowadays there are many different regulations (GDPR, CCPA, CPRA, etc.) that directly dictate what data you can collect and how you can use that data in your business. Violations of these regulations can have a significant negative impact to the business and the individuals they protect. Since data engineering teams are often the first stop for data in an organization, you need to be aware of them so you can design your solutions to protect that privacy and avoid any repercussions from storing and making restricted data available downstream.</li>
</ul>
</div>
<div class="section" id="analytical-skills">
<h3>Analytical Skills</h3>
<p>Some data engineers may specialize more on directly enabling analytical capabilities in their organization rather than handing off the data to data analysts or scientists to put into action. They may be embedded within a data analytics, finance, product, or marketing team for example and may be directly responsible for building reporting and highly specific business metrics on top of data that has already been processed upstream. This side of data engineering is branching more into what's being called &quot;Analytics Engineering&quot; but the role is nothing new. It has been around for a long time in larger companies as simply a product data engineer or a business intelligence engineer. This is quite common at places like Amazon and Meta, where data engineers tend be more involved with report and dashboard building as well as answering business questions with complex analytical queries. These data engineers tend to come from analytical backgrounds or classic SQL developer roles and rely more on the following skills.</p>
<ul class="simple">
<li><strong>Summarizing data and reporting</strong> - knowing how to build reports and massage the data to tell a story. The data you summarize in dashboards and reports will be consumed by business leaders who need accurate insights quickly in order to monitor the business and make strategic decisions. Don't underestimate this skill! It can take many years to build a knack for how to report data in a fast-paced business setting. It's also highly dependent on the type of organization you're in. For example, at Amazon, giant docs full of black-and-white Excel tables packed with numbers is the norm. At other places like Meta, the dense docs are replaced by streamlined and interactive dashboards that are always online and available for users to click through and digest.</li>
<li><strong>Data visualization</strong> - visualizing data is an effective way to digest it and tell a story. I probably don't need to tell you how common charts are in the business world because they're everywhere! So any engineer directly involved in communicating data needs to be aware of how best to visualize that data and which tools to use. This is also a commonly underestimated skill, but there are definitely right and wrong ways to visualize data. Knowing the difference can take a long time to master.</li>
<li><strong>Advanced SQL</strong> - in more analytical settings, the metrics can get quite complex. They may need to be derived by joining and aggregating many different tables across the company or by applying complicated business rules. As a result, the SQL required is often more advanced than in other areas of data engineering where you may just need SQL to do straightforward operations. They may involve aggregating over wide time periods like multiple quarters or years to report on trends over time, which typically involves larger amounts of data. The queries will be larger overall and may involve working through different aggregations of data as you combine all of the tables you need. So you need to be more cognizant of how to write performant SQL on the platform you're using and be able to break the problem up into multiple processing stages before coming up with a final query to produce the metrics.</li>
<li><strong>Basic statistical modeling and inference</strong> - drawing conclusions from data after it has been made available, curated, and visualized is a complex process by itself. How do you know whether a trend is expected or not? How can you tell with enough confidence that your product or business decision is actually making a difference in the outcome you're measuring given the data at hand? How much data do you even need to collect in order make a reasonable conclusion? How can you set up tests to validate your hypotheses? These are common problems in the field of statistics. Being familiar with this field and how to approach problems like these can take you a long way in reasoning about the data you have. Even if as a data engineer you're not a statistics expert like your coworkers over in data science, just having a fundamental understanding of it can help you produce better data products for your stakeholders.</li>
</ul>
</div>
<div class="section" id="advanced-engineering-skills">
<h3>Advanced Engineering Skills</h3>
<p>Now some data engineers may actually be building end-to-end data ingestion and processing systems that share similar characteristics to those traditionally tackled by software engineers. In this context, data engineers are like applied software engineers who focus specifically on data. They may be writing both the orchestration and business logic code to actually ingest and process raw data or building and maintaining the platforms that enable others in the organizations to do so. Other titles you may come across for these types of data engineers are software engineers - data, data platform engineers, and data software engineers. They need to be very familiar with the following concepts.</p>
<ul class="simple">
<li><strong>Python</strong> - a general purpose programming language that can literally do anything - data analytics, statistical computing, machine learning, object oriented programming, functional programming, web development, you name it. Many developers who grew up writing C/C++ and Java tend not to like Python because it's not statically typed and since it's an interpreted language that's <a class="reference external" href="https://decipheringbigdata.com/python-parallelism.html">not truly multi-threaded</a>, it's pretty slow. However, there is no getting around the fact that it's extremely popular and used heavily by most data teams these days. So if you're going to be working as an engineer with a focus on data, you need to know how to at least read Python, which is actually just like reading pseudocode :). Also, Airflow is a very popular orchestration tool that is written in Python and the way you typically write Airflow DAGs is with Python. The odds that you will come across this tool are very high.</li>
<li><strong>JVM language of choice</strong> - now Python is still somewhat of a second-class citizen with modern data processing frameworks. Tools in the Hadoop ecosystem and modern alternatives like Apache Spark, Flink, Beam, etc. are mostly developed using Java and Scala, which run on the Java virtual machine (JVM). The APIs and documentation tend to be available in Java or Scala first before they're ported over to Python. But usually that's not a big issue unless you're working with super new features. However, even if the APIs are similar and the same core features of these frameworks are available across the board, you will find that having an understanding of Java and Scala will help you discern how these frameworks work to process your data better, which is particularly handy during troubleshooting. Highly specialized data engineering teams that work on big data systems also tend to write their code in Java or Scala because it's a better choice for large code bases and typically results in more efficient code. These engineers may also come from traditional software engineering backgrounds where Java has dominated for a long time. The static typing also really helps when you're writing Spark code. It's better to find an error in your code at compile time than 30 minutes into your job.</li>
<li><strong>Distributed computing</strong> - all of the big data frameworks have this concept at their core. Understanding how data are distributed and processed in parallel is critical to truly grasp what's going on in your pipelines. If you have a good understanding of this, you'll be able to to design more efficient pipelines and have an easier time troubleshooting issues with them. So take the time to learn how the way you store your data, your partitioning schemes, indexing, and operations on the data affect how it's distributed at runtime.</li>
<li><strong>Streaming architectures</strong> - the world of data processing is moving more from a batch to a streaming paradigm. If you can write a pipeline as a streaming job, it can be run both on a set batch schedule or in real-time with just a simple config change instead of a rewrite of the code. The same is not true the other way around. Streaming jobs tend to be simpler and require less orchestration complexity. The streaming frameworks can simply listen to any new files continuously or since they last ran, compare with a checkpoint to determine what needs processing, and carry out the work without anything else telling them what to process. This approach is also becoming easier to adopt with platforms like Databricks doing a lot of the work for us behind the scenes.</li>
<li><strong>Devops</strong> - having an understanding of how to deploy your data systems and how to efficiently manage the associated infrastructure is critical. This means being familiar with continuous integration and deployment tools, infrastructure as code, containerization, monitoring tools, and security. I know some people may have dedicated devops teams that will handle a lot of this for them, but knowing how to tackle it yourself if you need to is important. I've seen too many data engineers manually creating AWS artifacts, manually editing IAM policies, not having a good grasp of all of the access tokens out in the wild, manually pushing their pipelines to production, etc. If you find yourself doing those types of tasks frequently, find some time to learn proper devops practices. It will save you a ton of time and headaches in the future!</li>
<li><strong>Cloud computing</strong> - this is another critical skill for engineers focusing on data. Unless you're working for a larger and more traditional company that may still run their data stack on on-premise hardware, odds are that you will be using some sort of cloud computing provider. The most commons providers are AWS, Azure, and Google Cloud. They all have specific products with different names but the core types of these products are the same - some sort of object storage like S3 and Google Storage, server instances like EC2, data warehouses like Redshift and BigQuery, serverless tools like Lambda and Azure Functions, and managed hadoop frameworks like EMR. These are typically the services most used for data engineering so picking one cloud provider and understanding how to use these services on their platform will take you far.</li>
<li><strong>Version control</strong> - then if you're working on any engineering team, you will need to know how to use version control software. There are many out there but the most common is Git. There are also multiple providers that offer Git services - Github, Bitbucket, Gitlab, etc. - but Git itself is the same open-source system common to all of them. It can take a while to grasp the concept and be exposed to all of the different scenarios you may encounter when working on a codebase that has many contributors, but putting in the effort to learn it and learn it well will make you a much more productive engineer. Version control is less common with the more analytical types of data engineering roles where engineers may just be writing their code directly into some UI, but it's increasingly being adopted more in those less technical roles with the help of frameworks like dbt.</li>
</ul>
</div>
</div>
<div class="section" id="how-to-learn-these-skills">
<h2>How to learn these skills</h2>
<p>It's a long list for sure, but don't be intimidated! Most professionals don't truly master all of these and it can take many years to be comfortable with a good chunk of this list. Start off with the fundamentals - learn how to write basic ETL using Python and SQL commands. Learn SQL well and write basic Python scripts to run simulated jobs. If you don't have a database handy, use libraries like Pandas in Python and text or JSON files that are freely available online from many sources like data.gov to practice your data wrangling skills. Ask yourself questions about the data and try to answer them. Explore as much data as you can using these tools. Practice is the only way to build up your data intuition skills. Also, try modeling the data. How can you structure it in order to make it easy for people to work with it? Try to apply concepts like normalization and star and snowflake schemas to your data.</p>
<p>Once you have a good grasp of the fundamentals, start playing around with more advanced engineering concepts like cloud computing. Amazon AWS is has plenty of free walkthroughs online on how to create your own databases, use S3 as object storage, query that data with Athena, launch EMR clusters, and pretty much everything you can think of to take your engineering knowledge to new heights. With EMR clusters, you can launch fully managed distributed environments to run Spark in. You don't need that to learn Spark of course because you can just install it locally and run it on your own computer. But, once you get a hang for the syntax and the data frame + Spark SQL APIs, using a tool like EMR is a good way to apply that knowledge on a larger scale of data. Google is your friend here! Just try it out and work your way through the common pitfalls.</p>
<p>Learn about partitioning data in object storage using Hive formatting and how Spark is able to discover the data and read it. Understand how the number of files and individual file sizes impact performance. That alone will take you farther than most data engineers out there who have dedicated teams in their companies worry about all of this infrastructure and data storage stuff for them or who don't really understand how the internals of Spark work.</p>
<p>Once you have a good grasp of batch processing pipelines on both databases and object storage using Python, SQL, and Spark, you may want to venture into writing streaming jobs. Learn how to work with the Spark Structured Streaming API using real-world feeds like what Twitter offers. You can even set up your own Kafka cluster or use AWS Kinesis and publish dummy data records into streams that you then consume and process with a streaming framework of your choice.</p>
<p>The important theme here is to just start! You have all of the resources available to you. There is no need to actually work in data to practice these skills. There is plenty of data online and many of the popular tools are completely open source. Below are a few of my posts to guide you in the right direction:</p>
<ul class="simple">
<li>An <a class="reference external" href="https://decipheringbigdata.com/mysql-yoursql-oursql-nosql.html">overview</a> on different data storage solutions you can learn about</li>
<li>An intro to <a class="reference external" href="https://decipheringbigdata.com/be-normal.html">normalization</a> and <a class="reference external" href="https://decipheringbigdata.com/stars-and-snowflakes.html">star schemas</a> that can come in handy when modeling data</li>
<li>Using <a class="reference external" href="https://decipheringbigdata.com/my-friend-sam.html">AWS Lambda</a> as an orchestration tool to ingest data from Twitter on a schedule</li>
<li>A basic <a class="reference external" href="https://decipheringbigdata.com/scala-spark-hello-world.html">Scala Spark template</a> to get started with</li>
</ul>
</div>


		<div id="article_meta">
				Category:
					<a href="https://decipheringbigdata.com/category/career.html">Career</a>
				<br />Tags:
					<a href="https://decipheringbigdata.com/tag/career.html">Career</a>
		</div>
	</article>

	<footer>
		<a href="https://decipheringbigdata.com/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
	</footer>


	</section>

</body>
</html>